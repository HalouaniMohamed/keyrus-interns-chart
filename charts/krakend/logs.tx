
==> Audit <==
|------------|--------------------------------|----------|----------|---------|---------------------|---------------------|
|  Command   |              Args              | Profile  |   User   | Version |     Start Time      |      End Time       |
|------------|--------------------------------|----------|----------|---------|---------------------|---------------------|
| start      |                                | minikube | halouani | v1.33.1 | 19 Jun 24 11:09 CET |                     |
| start      |                                | minikube | halouani | v1.33.1 | 19 Jun 24 11:10 CET |                     |
| start      |                                | minikube | halouani | v1.33.1 | 19 Jun 24 12:00 CET | 19 Jun 24 12:01 CET |
| kubectl    | -- get po -A                   | minikube | halouani | v1.33.1 | 19 Jun 24 12:13 CET |                     |
| kubectl    | -- get po -A                   | minikube | halouani | v1.33.1 | 19 Jun 24 12:14 CET |                     |
| dashboard  |                                | minikube | halouani | v1.33.1 | 19 Jun 24 12:14 CET |                     |
| kubectl    | -- get pod                     | minikube | halouani | v1.33.1 | 19 Jun 24 12:16 CET |                     |
| start      |                                | minikube | halouani | v1.33.1 | 19 Jun 24 12:17 CET | 19 Jun 24 12:17 CET |
| kubectl    | -- get po -A                   | minikube | halouani | v1.33.1 | 19 Jun 24 12:18 CET | 19 Jun 24 12:18 CET |
| kubectl    | -- pod pod                     | minikube | halouani | v1.33.1 | 19 Jun 24 12:18 CET |                     |
| kubectl    | -- get pod                     | minikube | halouani | v1.33.1 | 19 Jun 24 12:18 CET | 19 Jun 24 12:18 CET |
| kubectl    | -- get svc                     | minikube | halouani | v1.33.1 | 19 Jun 24 12:19 CET | 19 Jun 24 12:19 CET |
| kubectl    | -- get ns                      | minikube | halouani | v1.33.1 | 19 Jun 24 12:19 CET | 19 Jun 24 12:19 CET |
| kubectl    | -- get pod                     | minikube | halouani | v1.33.1 | 19 Jun 24 12:41 CET | 19 Jun 24 12:41 CET |
| kubectl    | -- get pod                     | minikube | halouani | v1.33.1 | 19 Jun 24 12:41 CET | 19 Jun 24 12:41 CET |
| kubectl    | -- get pod                     | minikube | halouani | v1.33.1 | 19 Jun 24 12:41 CET | 19 Jun 24 12:41 CET |
| kubectl    | -- delete pod                  | minikube | halouani | v1.33.1 | 19 Jun 24 12:41 CET | 19 Jun 24 12:41 CET |
|            | my-vaultwarden-release-0       |          |          |         |                     |                     |
| kubectl    | -- get pod                     | minikube | halouani | v1.33.1 | 19 Jun 24 12:42 CET | 19 Jun 24 12:42 CET |
| kubectl    | -- delete pod                  | minikube | halouani | v1.33.1 | 19 Jun 24 12:42 CET | 19 Jun 24 12:42 CET |
|            | my-vaultwarden-release-0       |          |          |         |                     |                     |
| kubectl    | -- get pod                     | minikube | halouani | v1.33.1 | 19 Jun 24 12:42 CET | 19 Jun 24 12:42 CET |
| kubectl    | -- get pod                     | minikube | halouani | v1.33.1 | 19 Jun 24 12:43 CET | 19 Jun 24 12:43 CET |
| kubectl    | -- get pod                     | minikube | halouani | v1.33.1 | 19 Jun 24 12:47 CET | 19 Jun 24 12:47 CET |
| kubectl    | -- create namespace vaulwarden | minikube | halouani | v1.33.1 | 19 Jun 24 12:47 CET | 19 Jun 24 12:47 CET |
| kubectl    | -- get ns                      | minikube | halouani | v1.33.1 | 19 Jun 24 12:47 CET | 19 Jun 24 12:47 CET |
| kubectl    | -- get pod -n vaulwarden       | minikube | halouani | v1.33.1 | 19 Jun 24 12:49 CET | 19 Jun 24 12:49 CET |
| kubectl    | -- delete pod                  | minikube | halouani | v1.33.1 | 19 Jun 24 12:49 CET |                     |
|            | vaulwarden-release-0           |          |          |         |                     |                     |
| start      |                                | minikube | halouani | v1.33.1 | 19 Jun 24 14:36 CET | 19 Jun 24 14:37 CET |
| kubectl    | -- get pod                     | minikube | halouani | v1.33.1 | 19 Jun 24 14:37 CET | 19 Jun 24 14:37 CET |
| ip         |                                | minikube | halouani | v1.33.1 | 19 Jun 24 16:00 CET | 19 Jun 24 16:00 CET |
| start      |                                | minikube | halouani | v1.33.1 | 20 Jun 24 12:06 CET | 20 Jun 24 12:06 CET |
| image      | load sample-ms:latest          | minikube | halouani | v1.33.1 | 20 Jun 24 12:11 CET |                     |
| image      | load sample-ms:latest          | minikube | halouani | v1.33.1 | 20 Jun 24 12:13 CET | 20 Jun 24 12:14 CET |
| image      | load sample-ms:latest          | minikube | halouani | v1.33.1 | 20 Jun 24 12:34 CET | 20 Jun 24 12:36 CET |
| docker-env |                                | minikube | halouani | v1.33.1 | 20 Jun 24 12:37 CET | 20 Jun 24 12:38 CET |
| ip         |                                | minikube | halouani | v1.33.1 | 20 Jun 24 12:54 CET | 20 Jun 24 12:54 CET |
| tunnel     |                                | minikube | halouani | v1.33.1 | 20 Jun 24 13:03 CET | 20 Jun 24 13:05 CET |
| ip         |                                | minikube | halouani | v1.33.1 | 20 Jun 24 13:08 CET | 20 Jun 24 13:08 CET |
| tunnel     |                                | minikube | halouani | v1.33.1 | 20 Jun 24 13:08 CET | 20 Jun 24 15:50 CET |
| start      |                                | minikube | halouani | v1.33.1 | 21 Jun 24 10:41 CET | 21 Jun 24 10:42 CET |
| start      | --driver=docker                | minikube | halouani | v1.33.1 | 21 Jun 24 15:32 CET | 21 Jun 24 15:33 CET |
| service    | prometheus-server-ext          | minikube | halouani | v1.33.1 | 21 Jun 24 15:35 CET |                     |
| service    | prometheus-server-ext -n       | minikube | halouani | v1.33.1 | 21 Jun 24 15:36 CET |                     |
|            | krakend-monitoring             |          |          |         |                     |                     |
| service    | prometheus-server-ext -n       | minikube | halouani | v1.33.1 | 21 Jun 24 15:37 CET | 21 Jun 24 15:37 CET |
|            | krakend-monitoring             |          |          |         |                     |                     |
| service    | prometheus-server-ext -n       | minikube | halouani | v1.33.1 | 21 Jun 24 15:41 CET | 21 Jun 24 15:41 CET |
|            | krakend-monitoring             |          |          |         |                     |                     |
| service    | grafana-ext                    | minikube | halouani | v1.33.1 | 21 Jun 24 15:46 CET |                     |
| service    | grafana-ext -n                 | minikube | halouani | v1.33.1 | 21 Jun 24 15:47 CET |                     |
|            | krakend-monitoring             |          |          |         |                     |                     |
| service    | grafana-ext                    | minikube | halouani | v1.33.1 | 21 Jun 24 15:52 CET |                     |
| service    | grafana-ext -n                 | minikube | halouani | v1.33.1 | 21 Jun 24 15:52 CET |                     |
|            | krakend-monitoring             |          |          |         |                     |                     |
| start      |                                | minikube | halouani | v1.33.1 | 24 Jun 24 10:33 CET | 24 Jun 24 10:33 CET |
| service    | krakend-ext                    | minikube | halouani | v1.33.1 | 24 Jun 24 10:56 CET |                     |
| service    | krakend-ext -n                 | minikube | halouani | v1.33.1 | 24 Jun 24 10:57 CET | 24 Jun 24 10:57 CET |
|            | krakend-monitoring             |          |          |         |                     |                     |
| service    | krakend-ext                    | minikube | halouani | v1.33.1 | 24 Jun 24 11:00 CET |                     |
| service    | krakend-ext -n                 | minikube | halouani | v1.33.1 | 24 Jun 24 11:00 CET | 24 Jun 24 11:00 CET |
|            | krakend-monitoring             |          |          |         |                     |                     |
| service    | grafana-ext                    | minikube | halouani | v1.33.1 | 24 Jun 24 11:07 CET |                     |
| service    | grafana-ext -n                 | minikube | halouani | v1.33.1 | 24 Jun 24 11:07 CET | 24 Jun 24 11:07 CET |
|            | krakend-monitoring             |          |          |         |                     |                     |
| ssh        |                                | minikube | halouani | v1.33.1 | 24 Jun 24 11:49 CET | 24 Jun 24 11:49 CET |
| ssh        |                                | minikube | halouani | v1.33.1 | 24 Jun 24 11:52 CET | 24 Jun 24 11:52 CET |
| ssh        |                                | minikube | halouani | v1.33.1 | 24 Jun 24 11:52 CET | 24 Jun 24 11:52 CET |
| service    | grafana-ext                    | minikube | halouani | v1.33.1 | 24 Jun 24 12:40 CET |                     |
| service    | grafana-ext -n                 | minikube | halouani | v1.33.1 | 24 Jun 24 12:40 CET |                     |
|            | krakend-monitoring             |          |          |         |                     |                     |
|------------|--------------------------------|----------|----------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2024/06/24 10:33:14
Running on machine: halouani-vm
Binary: Built with gc go1.22.1 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0624 10:33:14.113034    2565 out.go:291] Setting OutFile to fd 1 ...
I0624 10:33:14.113535    2565 out.go:343] isatty.IsTerminal(1) = true
I0624 10:33:14.113539    2565 out.go:304] Setting ErrFile to fd 2...
I0624 10:33:14.113541    2565 out.go:343] isatty.IsTerminal(2) = true
I0624 10:33:14.113678    2565 root.go:338] Updating PATH: /home/halouani/.minikube/bin
W0624 10:33:14.114485    2565 root.go:314] Error reading config file at /home/halouani/.minikube/config/config.json: open /home/halouani/.minikube/config/config.json: no such file or directory
I0624 10:33:14.115476    2565 out.go:298] Setting JSON to false
I0624 10:33:14.116538    2565 start.go:129] hostinfo: {"hostname":"halouani-vm","uptime":116,"bootTime":1719221478,"procs":326,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"22.04","kernelVersion":"6.5.0-41-generic","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"3a1803b4-d48b-465a-a1b5-aed438b95278"}
I0624 10:33:14.116575    2565 start.go:139] virtualization:  
I0624 10:33:14.117288    2565 out.go:177] üòÑ  minikube v1.33.1 on Ubuntu 22.04
I0624 10:33:14.119033    2565 notify.go:220] Checking for updates...
I0624 10:33:14.119380    2565 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0624 10:33:14.119934    2565 driver.go:392] Setting default libvirt URI to qemu:///system
I0624 10:33:14.299077    2565 docker.go:122] docker version: linux-26.1.4:Docker Engine - Community
I0624 10:33:14.299540    2565 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0624 10:33:15.010467    2565 info.go:266] docker info: {ID:456475e9-cd39-4965-8aa1-b6d24e04b71a Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:3 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:23 OomKillDisable:false NGoroutines:40 SystemTime:2024-06-24 10:33:14.995160108 +0100 CET LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.5.0-41-generic OperatingSystem:Ubuntu 22.04.4 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:2 MemTotal:4058660864 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:halouani-vm Labels:[] ExperimentalBuild:false ServerVersion:26.1.4 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:d2d58213f83a351ca8f528a95fbd145f5654e957 Expected:d2d58213f83a351ca8f528a95fbd145f5654e957} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.14.1] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.27.1]] Warnings:<nil>}}
I0624 10:33:15.010584    2565 docker.go:295] overlay module found
I0624 10:33:15.011262    2565 out.go:177] ‚ú®  Using the docker driver based on existing profile
I0624 10:33:15.011978    2565 start.go:297] selected driver: docker
I0624 10:33:15.011983    2565 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/halouani:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0624 10:33:15.012041    2565 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0624 10:33:15.012095    2565 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0624 10:33:15.107538    2565 info.go:266] docker info: {ID:456475e9-cd39-4965-8aa1-b6d24e04b71a Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:3 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:23 OomKillDisable:false NGoroutines:40 SystemTime:2024-06-24 10:33:15.093863959 +0100 CET LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.5.0-41-generic OperatingSystem:Ubuntu 22.04.4 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:2 MemTotal:4058660864 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:halouani-vm Labels:[] ExperimentalBuild:false ServerVersion:26.1.4 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:d2d58213f83a351ca8f528a95fbd145f5654e957 Expected:d2d58213f83a351ca8f528a95fbd145f5654e957} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.14.1] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.27.1]] Warnings:<nil>}}
I0624 10:33:15.108274    2565 cni.go:84] Creating CNI manager for ""
I0624 10:33:15.108296    2565 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0624 10:33:15.108777    2565 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/halouani:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0624 10:33:15.110897    2565 out.go:177] üëç  Starting "minikube" primary control-plane node in "minikube" cluster
I0624 10:33:15.112315    2565 cache.go:121] Beginning downloading kic base image for docker with docker
I0624 10:33:15.113413    2565 out.go:177] üöú  Pulling base image v0.0.44 ...
I0624 10:33:15.114682    2565 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0624 10:33:15.114808    2565 preload.go:147] Found local preload: /home/halouani/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4
I0624 10:33:15.114818    2565 cache.go:56] Caching tarball of preloaded images
I0624 10:33:15.115038    2565 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e in local docker daemon
I0624 10:33:15.115316    2565 preload.go:173] Found /home/halouani/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0624 10:33:15.115343    2565 cache.go:59] Finished verifying existence of preloaded tar for v1.30.0 on docker
I0624 10:33:15.115485    2565 profile.go:143] Saving config to /home/halouani/.minikube/profiles/minikube/config.json ...
I0624 10:33:15.153394    2565 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e in local docker daemon, skipping pull
I0624 10:33:15.153405    2565 cache.go:144] gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e exists in daemon, skipping load
I0624 10:33:15.153416    2565 cache.go:194] Successfully downloaded all kic artifacts
I0624 10:33:15.153443    2565 start.go:360] acquireMachinesLock for minikube: {Name:mk831b44d29093758a5cdc17d2c4be0e42b720a8 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0624 10:33:15.153534    2565 start.go:364] duration metric: took 76.973¬µs to acquireMachinesLock for "minikube"
I0624 10:33:15.153545    2565 start.go:96] Skipping create...Using existing machine configuration
I0624 10:33:15.153551    2565 fix.go:54] fixHost starting: 
I0624 10:33:15.153720    2565 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0624 10:33:15.221726    2565 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0624 10:33:15.221750    2565 fix.go:138] unexpected machine state, will restart: <nil>
I0624 10:33:15.222522    2565 out.go:177] üîÑ  Restarting existing docker container for "minikube" ...
I0624 10:33:15.222982    2565 cli_runner.go:164] Run: docker start minikube
I0624 10:33:16.464064    2565 cli_runner.go:217] Completed: docker start minikube: (1.24105762s)
I0624 10:33:16.464195    2565 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0624 10:33:16.484338    2565 kic.go:430] container "minikube" state is running.
I0624 10:33:16.484801    2565 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0624 10:33:16.503828    2565 profile.go:143] Saving config to /home/halouani/.minikube/profiles/minikube/config.json ...
I0624 10:33:16.504245    2565 machine.go:94] provisionDockerMachine start ...
I0624 10:33:16.504295    2565 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0624 10:33:16.535092    2565 main.go:141] libmachine: Using SSH client type: native
I0624 10:33:16.535937    2565 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0624 10:33:16.535949    2565 main.go:141] libmachine: About to run SSH command:
hostname
I0624 10:33:16.536505    2565 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:49676->127.0.0.1:32772: read: connection reset by peer
I0624 10:33:19.772428    2565 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0624 10:33:19.772525    2565 ubuntu.go:169] provisioning hostname "minikube"
I0624 10:33:19.772580    2565 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0624 10:33:19.807422    2565 main.go:141] libmachine: Using SSH client type: native
I0624 10:33:19.807837    2565 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0624 10:33:19.807852    2565 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0624 10:33:20.043100    2565 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0624 10:33:20.043191    2565 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0624 10:33:20.116043    2565 main.go:141] libmachine: Using SSH client type: native
I0624 10:33:20.116216    2565 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0624 10:33:20.116224    2565 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0624 10:33:20.274849    2565 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0624 10:33:20.274871    2565 ubuntu.go:175] set auth options {CertDir:/home/halouani/.minikube CaCertPath:/home/halouani/.minikube/certs/ca.pem CaPrivateKeyPath:/home/halouani/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/halouani/.minikube/machines/server.pem ServerKeyPath:/home/halouani/.minikube/machines/server-key.pem ClientKeyPath:/home/halouani/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/halouani/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/halouani/.minikube}
I0624 10:33:20.274887    2565 ubuntu.go:177] setting up certificates
I0624 10:33:20.274893    2565 provision.go:84] configureAuth start
I0624 10:33:20.274931    2565 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0624 10:33:20.382108    2565 provision.go:143] copyHostCerts
I0624 10:33:20.383110    2565 exec_runner.go:144] found /home/halouani/.minikube/ca.pem, removing ...
I0624 10:33:20.383150    2565 exec_runner.go:203] rm: /home/halouani/.minikube/ca.pem
I0624 10:33:20.383227    2565 exec_runner.go:151] cp: /home/halouani/.minikube/certs/ca.pem --> /home/halouani/.minikube/ca.pem (1082 bytes)
I0624 10:33:20.384017    2565 exec_runner.go:144] found /home/halouani/.minikube/cert.pem, removing ...
I0624 10:33:20.384031    2565 exec_runner.go:203] rm: /home/halouani/.minikube/cert.pem
I0624 10:33:20.384079    2565 exec_runner.go:151] cp: /home/halouani/.minikube/certs/cert.pem --> /home/halouani/.minikube/cert.pem (1127 bytes)
I0624 10:33:20.384918    2565 exec_runner.go:144] found /home/halouani/.minikube/key.pem, removing ...
I0624 10:33:20.384929    2565 exec_runner.go:203] rm: /home/halouani/.minikube/key.pem
I0624 10:33:20.384975    2565 exec_runner.go:151] cp: /home/halouani/.minikube/certs/key.pem --> /home/halouani/.minikube/key.pem (1675 bytes)
I0624 10:33:20.385892    2565 provision.go:117] generating server cert: /home/halouani/.minikube/machines/server.pem ca-key=/home/halouani/.minikube/certs/ca.pem private-key=/home/halouani/.minikube/certs/ca-key.pem org=halouani.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0624 10:33:20.532352    2565 provision.go:177] copyRemoteCerts
I0624 10:33:20.532484    2565 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0624 10:33:20.532530    2565 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0624 10:33:20.557978    2565 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/halouani/.minikube/machines/minikube/id_rsa Username:docker}
I0624 10:33:20.675681    2565 ssh_runner.go:362] scp /home/halouani/.minikube/machines/server.pem --> /etc/docker/server.pem (1184 bytes)
I0624 10:33:20.731041    2565 ssh_runner.go:362] scp /home/halouani/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0624 10:33:20.759523    2565 ssh_runner.go:362] scp /home/halouani/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1082 bytes)
I0624 10:33:20.789050    2565 provision.go:87] duration metric: took 514.144747ms to configureAuth
I0624 10:33:20.789070    2565 ubuntu.go:193] setting minikube options for container-runtime
I0624 10:33:20.789226    2565 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0624 10:33:20.789280    2565 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0624 10:33:20.841679    2565 main.go:141] libmachine: Using SSH client type: native
I0624 10:33:20.842280    2565 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0624 10:33:20.842336    2565 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0624 10:33:21.019319    2565 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0624 10:33:21.019331    2565 ubuntu.go:71] root file system type: overlay
I0624 10:33:21.019417    2565 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0624 10:33:21.019462    2565 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0624 10:33:21.037226    2565 main.go:141] libmachine: Using SSH client type: native
I0624 10:33:21.037433    2565 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0624 10:33:21.037496    2565 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0624 10:33:21.231569    2565 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0624 10:33:21.231773    2565 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0624 10:33:21.259111    2565 main.go:141] libmachine: Using SSH client type: native
I0624 10:33:21.260104    2565 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0624 10:33:21.260115    2565 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0624 10:33:21.460002    2565 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0624 10:33:21.460023    2565 machine.go:97] duration metric: took 4.955765519s to provisionDockerMachine
I0624 10:33:21.460037    2565 start.go:293] postStartSetup for "minikube" (driver="docker")
I0624 10:33:21.460073    2565 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0624 10:33:21.460175    2565 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0624 10:33:21.460229    2565 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0624 10:33:21.487187    2565 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/halouani/.minikube/machines/minikube/id_rsa Username:docker}
I0624 10:33:21.608993    2565 ssh_runner.go:195] Run: cat /etc/os-release
I0624 10:33:21.616151    2565 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0624 10:33:21.616190    2565 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0624 10:33:21.616203    2565 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0624 10:33:21.616213    2565 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I0624 10:33:21.616236    2565 filesync.go:126] Scanning /home/halouani/.minikube/addons for local assets ...
I0624 10:33:21.616870    2565 filesync.go:126] Scanning /home/halouani/.minikube/files for local assets ...
I0624 10:33:21.617714    2565 start.go:296] duration metric: took 157.661106ms for postStartSetup
I0624 10:33:21.617773    2565 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0624 10:33:21.617897    2565 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0624 10:33:21.648330    2565 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/halouani/.minikube/machines/minikube/id_rsa Username:docker}
I0624 10:33:21.750965    2565 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0624 10:33:21.758772    2565 fix.go:56] duration metric: took 6.605221225s for fixHost
I0624 10:33:21.758784    2565 start.go:83] releasing machines lock for "minikube", held for 6.605244681s
I0624 10:33:21.758823    2565 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0624 10:33:21.815468    2565 ssh_runner.go:195] Run: cat /version.json
I0624 10:33:21.815505    2565 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0624 10:33:21.815728    2565 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0624 10:33:21.815775    2565 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0624 10:33:21.866134    2565 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/halouani/.minikube/machines/minikube/id_rsa Username:docker}
I0624 10:33:21.917125    2565 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/halouani/.minikube/machines/minikube/id_rsa Username:docker}
I0624 10:33:22.343546    2565 ssh_runner.go:195] Run: systemctl --version
I0624 10:33:22.361444    2565 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0624 10:33:22.369744    2565 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0624 10:33:22.403643    2565 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0624 10:33:22.403739    2565 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0624 10:33:22.421780    2565 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0624 10:33:22.421794    2565 start.go:494] detecting cgroup driver to use...
I0624 10:33:22.421816    2565 detect.go:199] detected "systemd" cgroup driver on host os
I0624 10:33:22.421990    2565 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0624 10:33:22.457587    2565 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0624 10:33:22.476172    2565 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0624 10:33:22.492007    2565 containerd.go:146] configuring containerd to use "systemd" as cgroup driver...
I0624 10:33:22.492105    2565 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I0624 10:33:22.507129    2565 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0624 10:33:22.517859    2565 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0624 10:33:22.527605    2565 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0624 10:33:22.538760    2565 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0624 10:33:22.550821    2565 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0624 10:33:22.565270    2565 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0624 10:33:22.579574    2565 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0624 10:33:22.598685    2565 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0624 10:33:22.623819    2565 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0624 10:33:22.639510    2565 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0624 10:33:22.766925    2565 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0624 10:33:22.937600    2565 start.go:494] detecting cgroup driver to use...
I0624 10:33:22.937627    2565 detect.go:199] detected "systemd" cgroup driver on host os
I0624 10:33:22.937689    2565 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0624 10:33:22.968235    2565 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0624 10:33:22.968283    2565 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0624 10:33:22.984948    2565 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0624 10:33:23.008221    2565 ssh_runner.go:195] Run: which cri-dockerd
I0624 10:33:23.012657    2565 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0624 10:33:23.027074    2565 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0624 10:33:23.051115    2565 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0624 10:33:23.197513    2565 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0624 10:33:23.407355    2565 docker.go:574] configuring docker to use "systemd" as cgroup driver...
I0624 10:33:23.407461    2565 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (129 bytes)
I0624 10:33:23.464243    2565 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0624 10:33:23.580780    2565 ssh_runner.go:195] Run: sudo systemctl restart docker
I0624 10:33:24.986999    2565 ssh_runner.go:235] Completed: sudo systemctl restart docker: (1.406162929s)
I0624 10:33:24.987088    2565 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0624 10:33:24.998668    2565 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0624 10:33:25.010897    2565 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0624 10:33:25.024497    2565 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0624 10:33:25.182858    2565 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0624 10:33:25.302525    2565 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0624 10:33:25.463607    2565 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0624 10:33:25.508284    2565 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0624 10:33:25.538469    2565 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0624 10:33:25.690040    2565 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0624 10:33:26.421068    2565 start.go:541] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0624 10:33:26.421172    2565 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0624 10:33:26.427044    2565 start.go:562] Will wait 60s for crictl version
I0624 10:33:26.427091    2565 ssh_runner.go:195] Run: which crictl
I0624 10:33:26.433134    2565 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0624 10:33:26.661239    2565 start.go:578] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  26.1.1
RuntimeApiVersion:  v1
I0624 10:33:26.661385    2565 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0624 10:33:26.793041    2565 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0624 10:33:26.842298    2565 out.go:204] üê≥  Preparing Kubernetes v1.30.0 on Docker 26.1.1 ...
I0624 10:33:26.844038    2565 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0624 10:33:26.865648    2565 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0624 10:33:26.870141    2565 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0624 10:33:26.892877    2565 kubeadm.go:877] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/halouani:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0624 10:33:26.893114    2565 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0624 10:33:26.893217    2565 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0624 10:33:26.936059    2565 docker.go:685] Got preloaded images: -- stdout --
sample-ms:latest
<none>:<none>
quay.io/prometheus/prometheus:v2.53.0
bitnami/postgresql:16.3.0-debian-12-r14
quay.io/prometheus/node-exporter:v1.8.1
quay.io/prometheus-operator/prometheus-config-reloader:v0.74.0
grafana/grafana:11.0.0
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
registry.k8s.io/ingress-nginx/controller:<none>
registry.k8s.io/ingress-nginx/kube-webhook-certgen:<none>
quay.io/prometheus/pushgateway:v1.8.0
registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.12.0
quay.io/prometheus/alertmanager:v0.27.0
registry.k8s.io/etcd:3.5.12-0
vaultwarden/server:1.30.3-alpine
registry.k8s.io/coredns/coredns:v1.11.1
busybox:latest
psono/psono-combo:3.3.2-2.3.3-1.6.6
registry.k8s.io/pause:3.9
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0624 10:33:26.936073    2565 docker.go:615] Images already preloaded, skipping extraction
I0624 10:33:26.936124    2565 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0624 10:33:26.954938    2565 docker.go:685] Got preloaded images: -- stdout --
sample-ms:latest
<none>:<none>
quay.io/prometheus/prometheus:v2.53.0
bitnami/postgresql:16.3.0-debian-12-r14
quay.io/prometheus/node-exporter:v1.8.1
quay.io/prometheus-operator/prometheus-config-reloader:v0.74.0
grafana/grafana:11.0.0
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
registry.k8s.io/ingress-nginx/controller:<none>
registry.k8s.io/ingress-nginx/kube-webhook-certgen:<none>
quay.io/prometheus/pushgateway:v1.8.0
registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.12.0
quay.io/prometheus/alertmanager:v0.27.0
registry.k8s.io/etcd:3.5.12-0
vaultwarden/server:1.30.3-alpine
registry.k8s.io/coredns/coredns:v1.11.1
busybox:latest
psono/psono-combo:3.3.2-2.3.3-1.6.6
registry.k8s.io/pause:3.9
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0624 10:33:26.954950    2565 cache_images.go:84] Images are preloaded, skipping loading
I0624 10:33:26.954957    2565 kubeadm.go:928] updating node { 192.168.49.2 8443 v1.30.0 docker true true} ...
I0624 10:33:26.955105    2565 kubeadm.go:940] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.30.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0624 10:33:26.955161    2565 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0624 10:33:27.626702    2565 cni.go:84] Creating CNI manager for ""
I0624 10:33:27.626725    2565 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0624 10:33:27.626736    2565 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0624 10:33:27.626756    2565 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.30.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0624 10:33:27.626892    2565 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.30.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0624 10:33:27.626957    2565 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.30.0
I0624 10:33:27.646049    2565 binaries.go:44] Found k8s binaries, skipping transfer
I0624 10:33:27.646086    2565 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0624 10:33:27.666454    2565 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0624 10:33:27.706805    2565 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0624 10:33:27.746905    2565 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2149 bytes)
I0624 10:33:27.788328    2565 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0624 10:33:27.792456    2565 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0624 10:33:27.813816    2565 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0624 10:33:27.961990    2565 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0624 10:33:28.017212    2565 certs.go:68] Setting up /home/halouani/.minikube/profiles/minikube for IP: 192.168.49.2
I0624 10:33:28.017226    2565 certs.go:194] generating shared ca certs ...
I0624 10:33:28.017244    2565 certs.go:226] acquiring lock for ca certs: {Name:mkfee6047eb75be6c478b03559904c247c7cae46 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0624 10:33:28.018270    2565 certs.go:235] skipping valid "minikubeCA" ca cert: /home/halouani/.minikube/ca.key
I0624 10:33:28.019282    2565 certs.go:235] skipping valid "proxyClientCA" ca cert: /home/halouani/.minikube/proxy-client-ca.key
I0624 10:33:28.019300    2565 certs.go:256] generating profile certs ...
I0624 10:33:28.020213    2565 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": /home/halouani/.minikube/profiles/minikube/client.key
I0624 10:33:28.021010    2565 certs.go:359] skipping valid signed profile cert regeneration for "minikube": /home/halouani/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I0624 10:33:28.021705    2565 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": /home/halouani/.minikube/profiles/minikube/proxy-client.key
I0624 10:33:28.021965    2565 certs.go:484] found cert: /home/halouani/.minikube/certs/ca-key.pem (1675 bytes)
I0624 10:33:28.022021    2565 certs.go:484] found cert: /home/halouani/.minikube/certs/ca.pem (1082 bytes)
I0624 10:33:28.022073    2565 certs.go:484] found cert: /home/halouani/.minikube/certs/cert.pem (1127 bytes)
I0624 10:33:28.022111    2565 certs.go:484] found cert: /home/halouani/.minikube/certs/key.pem (1675 bytes)
I0624 10:33:28.023717    2565 ssh_runner.go:362] scp /home/halouani/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0624 10:33:28.110709    2565 ssh_runner.go:362] scp /home/halouani/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0624 10:33:28.181403    2565 ssh_runner.go:362] scp /home/halouani/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0624 10:33:28.237533    2565 ssh_runner.go:362] scp /home/halouani/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0624 10:33:28.299444    2565 ssh_runner.go:362] scp /home/halouani/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0624 10:33:28.350296    2565 ssh_runner.go:362] scp /home/halouani/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0624 10:33:28.380700    2565 ssh_runner.go:362] scp /home/halouani/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0624 10:33:28.447008    2565 ssh_runner.go:362] scp /home/halouani/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0624 10:33:28.532773    2565 ssh_runner.go:362] scp /home/halouani/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0624 10:33:28.584371    2565 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0624 10:33:28.608816    2565 ssh_runner.go:195] Run: openssl version
I0624 10:33:28.632008    2565 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0624 10:33:28.656041    2565 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0624 10:33:28.660863    2565 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Jun 19 10:15 /usr/share/ca-certificates/minikubeCA.pem
I0624 10:33:28.660933    2565 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0624 10:33:28.670907    2565 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0624 10:33:28.681984    2565 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0624 10:33:28.687692    2565 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0624 10:33:28.700982    2565 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0624 10:33:28.721324    2565 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0624 10:33:28.742607    2565 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0624 10:33:28.769884    2565 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0624 10:33:28.802370    2565 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0624 10:33:28.830671    2565 kubeadm.go:391] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/halouani:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0624 10:33:28.830912    2565 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0624 10:33:28.897631    2565 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
W0624 10:33:28.941548    2565 kubeadm.go:404] apiserver tunnel failed: apiserver port not set
I0624 10:33:28.941567    2565 kubeadm.go:407] found existing configuration files, will attempt cluster restart
I0624 10:33:28.941588    2565 kubeadm.go:587] restartPrimaryControlPlane start ...
I0624 10:33:28.941655    2565 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0624 10:33:28.969005    2565 kubeadm.go:129] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0624 10:33:28.972536    2565 kubeconfig.go:125] found "minikube" server: "https://192.168.49.2:8443"
I0624 10:33:28.998954    2565 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0624 10:33:29.016610    2565 kubeadm.go:624] The running cluster does not require reconfiguration: 192.168.49.2
I0624 10:33:29.016636    2565 kubeadm.go:591] duration metric: took 75.042215ms to restartPrimaryControlPlane
I0624 10:33:29.016644    2565 kubeadm.go:393] duration metric: took 185.995101ms to StartCluster
I0624 10:33:29.016660    2565 settings.go:142] acquiring lock: {Name:mkc9f990a90c746061f2fa63de772a18d6bf6af5 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0624 10:33:29.016763    2565 settings.go:150] Updating kubeconfig:  /home/halouani/.kube/config
I0624 10:33:29.017882    2565 lock.go:35] WriteFile acquiring /home/halouani/.kube/config: {Name:mkfd76c7bd61ae6c9f4dead03bcd1f15b270e181 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0624 10:33:29.018296    2565 start.go:234] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0624 10:33:29.019724    2565 out.go:177] üîé  Verifying Kubernetes components...
I0624 10:33:29.018460    2565 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0624 10:33:29.018475    2565 addons.go:502] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false yakd:false]
I0624 10:33:29.020602    2565 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0624 10:33:29.020604    2565 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0624 10:33:29.020663    2565 addons.go:69] Setting dashboard=true in profile "minikube"
I0624 10:33:29.020681    2565 addons.go:234] Setting addon dashboard=true in "minikube"
W0624 10:33:29.020719    2565 addons.go:243] addon dashboard should already be in state true
I0624 10:33:29.020729    2565 addons.go:234] Setting addon storage-provisioner=true in "minikube"
W0624 10:33:29.020736    2565 addons.go:243] addon storage-provisioner should already be in state true
I0624 10:33:29.020747    2565 host.go:66] Checking if "minikube" exists ...
I0624 10:33:29.020750    2565 host.go:66] Checking if "minikube" exists ...
I0624 10:33:29.021180    2565 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0624 10:33:29.021197    2565 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0624 10:33:29.026614    2565 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0624 10:33:29.026637    2565 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0624 10:33:29.026929    2565 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0624 10:33:29.239569    2565 out.go:177]     ‚ñ™ Using image docker.io/kubernetesui/dashboard:v2.7.0
I0624 10:33:29.240884    2565 addons.go:234] Setting addon default-storageclass=true in "minikube"
W0624 10:33:29.240897    2565 addons.go:243] addon default-storageclass should already be in state true
I0624 10:33:29.240920    2565 host.go:66] Checking if "minikube" exists ...
I0624 10:33:29.242428    2565 out.go:177]     ‚ñ™ Using image docker.io/kubernetesui/metrics-scraper:v1.0.8
I0624 10:33:29.241506    2565 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0624 10:33:29.243549    2565 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0624 10:33:29.243646    2565 addons.go:426] installing /etc/kubernetes/addons/dashboard-ns.yaml
I0624 10:33:29.243655    2565 ssh_runner.go:362] scp dashboard/dashboard-ns.yaml --> /etc/kubernetes/addons/dashboard-ns.yaml (759 bytes)
I0624 10:33:29.243704    2565 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0624 10:33:29.244784    2565 addons.go:426] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0624 10:33:29.244791    2565 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0624 10:33:29.244834    2565 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0624 10:33:29.353850    2565 addons.go:426] installing /etc/kubernetes/addons/storageclass.yaml
I0624 10:33:29.353865    2565 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0624 10:33:29.353922    2565 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0624 10:33:29.366445    2565 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0624 10:33:29.386407    2565 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/halouani/.minikube/machines/minikube/id_rsa Username:docker}
I0624 10:33:29.396085    2565 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/halouani/.minikube/machines/minikube/id_rsa Username:docker}
I0624 10:33:29.414608    2565 api_server.go:52] waiting for apiserver process to appear ...
I0624 10:33:29.414654    2565 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0624 10:33:29.415437    2565 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/halouani/.minikube/machines/minikube/id_rsa Username:docker}
I0624 10:33:29.725993    2565 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0624 10:33:29.814501    2565 addons.go:426] installing /etc/kubernetes/addons/dashboard-clusterrole.yaml
I0624 10:33:29.814518    2565 ssh_runner.go:362] scp dashboard/dashboard-clusterrole.yaml --> /etc/kubernetes/addons/dashboard-clusterrole.yaml (1001 bytes)
I0624 10:33:29.826031    2565 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0624 10:33:29.888571    2565 addons.go:426] installing /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml
I0624 10:33:29.888587    2565 ssh_runner.go:362] scp dashboard/dashboard-clusterrolebinding.yaml --> /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml (1018 bytes)
I0624 10:33:29.915970    2565 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0624 10:33:29.990707    2565 addons.go:426] installing /etc/kubernetes/addons/dashboard-configmap.yaml
I0624 10:33:29.990721    2565 ssh_runner.go:362] scp dashboard/dashboard-configmap.yaml --> /etc/kubernetes/addons/dashboard-configmap.yaml (837 bytes)
I0624 10:33:30.141565    2565 addons.go:426] installing /etc/kubernetes/addons/dashboard-dp.yaml
I0624 10:33:30.141578    2565 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-dp.yaml (4288 bytes)
I0624 10:33:30.395723    2565 addons.go:426] installing /etc/kubernetes/addons/dashboard-role.yaml
I0624 10:33:30.395741    2565 ssh_runner.go:362] scp dashboard/dashboard-role.yaml --> /etc/kubernetes/addons/dashboard-role.yaml (1724 bytes)
I0624 10:33:30.479403    2565 addons.go:426] installing /etc/kubernetes/addons/dashboard-rolebinding.yaml
I0624 10:33:30.479420    2565 ssh_runner.go:362] scp dashboard/dashboard-rolebinding.yaml --> /etc/kubernetes/addons/dashboard-rolebinding.yaml (1046 bytes)
I0624 10:33:30.579868    2565 addons.go:426] installing /etc/kubernetes/addons/dashboard-sa.yaml
I0624 10:33:30.579893    2565 ssh_runner.go:362] scp dashboard/dashboard-sa.yaml --> /etc/kubernetes/addons/dashboard-sa.yaml (837 bytes)
I0624 10:33:30.917898    2565 addons.go:426] installing /etc/kubernetes/addons/dashboard-secret.yaml
I0624 10:33:30.917915    2565 ssh_runner.go:362] scp dashboard/dashboard-secret.yaml --> /etc/kubernetes/addons/dashboard-secret.yaml (1389 bytes)
I0624 10:33:31.215441    2565 addons.go:426] installing /etc/kubernetes/addons/dashboard-svc.yaml
I0624 10:33:31.215462    2565 ssh_runner.go:362] scp dashboard/dashboard-svc.yaml --> /etc/kubernetes/addons/dashboard-svc.yaml (1294 bytes)
I0624 10:33:31.295955    2565 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (1.569920197s)
W0624 10:33:31.295980    2565 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0624 10:33:31.295998    2565 retry.go:31] will retry after 125.025844ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0624 10:33:31.296004    2565 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (1.469955319s)
W0624 10:33:31.296015    2565 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0624 10:33:31.296022    2565 retry.go:31] will retry after 215.31566ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0624 10:33:31.296026    2565 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (1.380039643s)
I0624 10:33:31.296071    2565 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0624 10:33:31.414882    2565 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0624 10:33:31.414930    2565 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I0624 10:33:31.421728    2565 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0624 10:33:31.511713    2565 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0624 10:33:32.238083    2565 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0624 10:33:32.238179    2565 retry.go:31] will retry after 248.162301ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0624 10:33:32.238192    2565 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0624 10:33:32.238274    2565 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0624 10:33:32.238282    2565 retry.go:31] will retry after 419.586807ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0624 10:33:32.238335    2565 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0624 10:33:32.238341    2565 retry.go:31] will retry after 463.167266ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0624 10:33:32.265559    2565 api_server.go:72] duration metric: took 3.247203232s to wait for apiserver process to appear ...
I0624 10:33:32.265584    2565 api_server.go:88] waiting for apiserver healthz status ...
I0624 10:33:32.265620    2565 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0624 10:33:32.266205    2565 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0624 10:33:32.486659    2565 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I0624 10:33:32.658813    2565 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0624 10:33:32.680871    2565 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0624 10:33:32.680893    2565 retry.go:31] will retry after 247.603768ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0624 10:33:32.702184    2565 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0624 10:33:32.765784    2565 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0624 10:33:32.766410    2565 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
W0624 10:33:32.847309    2565 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0624 10:33:32.847332    2565 retry.go:31] will retry after 281.359017ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0624 10:33:32.929742    2565 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
W0624 10:33:33.001305    2565 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0624 10:33:33.001336    2565 retry.go:31] will retry after 811.820174ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0624 10:33:33.129872    2565 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0624 10:33:33.159658    2565 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0624 10:33:33.159692    2565 retry.go:31] will retry after 589.661781ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0624 10:33:33.265950    2565 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0624 10:33:33.266389    2565 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
W0624 10:33:33.344838    2565 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0624 10:33:33.344860    2565 retry.go:31] will retry after 1.143282976s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0624 10:33:33.749738    2565 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I0624 10:33:33.766225    2565 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0624 10:33:33.814094    2565 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0624 10:33:34.488711    2565 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0624 10:33:38.599136    2565 api_server.go:279] https://192.168.49.2:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0624 10:33:38.599188    2565 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0624 10:33:38.599210    2565 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0624 10:33:38.798754    2565 api_server.go:279] https://192.168.49.2:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0624 10:33:38.798791    2565 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0624 10:33:38.798802    2565 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0624 10:33:38.866690    2565 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[-]poststarthook/crd-informer-synced failed: reason withheld
[-]poststarthook/start-service-ip-repair-controllers failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0624 10:33:38.866716    2565 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[-]poststarthook/crd-informer-synced failed: reason withheld
[-]poststarthook/start-service-ip-repair-controllers failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0624 10:33:39.266244    2565 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0624 10:33:39.317022    2565 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0624 10:33:39.317060    2565 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0624 10:33:39.766717    2565 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0624 10:33:39.787644    2565 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0624 10:33:39.787679    2565 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0624 10:33:40.266156    2565 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0624 10:33:40.284938    2565 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0624 10:33:40.284958    2565 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0624 10:33:40.766811    2565 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0624 10:33:40.771341    2565 api_server.go:279] https://192.168.49.2:8443/healthz returned 200:
ok
I0624 10:33:40.797959    2565 api_server.go:141] control plane version: v1.30.0
I0624 10:33:40.797996    2565 api_server.go:131] duration metric: took 8.532386283s to wait for apiserver health ...
I0624 10:33:40.798023    2565 system_pods.go:43] waiting for kube-system pods to appear ...
I0624 10:33:40.839110    2565 system_pods.go:59] 8 kube-system pods found
I0624 10:33:40.839164    2565 system_pods.go:61] "coredns-7db6d8ff4d-5htm8" [f3fdd478-ea5d-4764-b0af-da041c6ed964] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0624 10:33:40.839172    2565 system_pods.go:61] "coredns-7db6d8ff4d-pvdxk" [711b0b76-d8f7-4569-b9ff-a4c09fed6bba] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0624 10:33:40.839178    2565 system_pods.go:61] "etcd-minikube" [5afc88ba-c4ef-4436-bff7-1b1a42a1d3fd] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0624 10:33:40.839184    2565 system_pods.go:61] "kube-apiserver-minikube" [898ccf41-cd96-40ba-9bec-d0e1e94d7005] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0624 10:33:40.839191    2565 system_pods.go:61] "kube-controller-manager-minikube" [85cad400-4767-4bc3-b6d2-05d63b7b1237] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0624 10:33:40.839196    2565 system_pods.go:61] "kube-proxy-48lpn" [f5cf5788-cbfa-4f4f-8ca1-62073e6aec50] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0624 10:33:40.839202    2565 system_pods.go:61] "kube-scheduler-minikube" [59d7dd88-dbcc-4baf-99af-a97d41181bcb] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0624 10:33:40.839207    2565 system_pods.go:61] "storage-provisioner" [1091fd4a-9b7e-4f27-b9ca-5ee1668e8046] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0624 10:33:40.839212    2565 system_pods.go:74] duration metric: took 41.182524ms to wait for pod list to return data ...
I0624 10:33:40.839223    2565 kubeadm.go:576] duration metric: took 11.820872953s to wait for: map[apiserver:true system_pods:true]
I0624 10:33:40.839234    2565 node_conditions.go:102] verifying NodePressure condition ...
I0624 10:33:40.850924    2565 node_conditions.go:122] node storage ephemeral capacity is 50770432Ki
I0624 10:33:40.850958    2565 node_conditions.go:123] node cpu capacity is 2
I0624 10:33:40.850972    2565 node_conditions.go:105] duration metric: took 11.734185ms to run NodePressure ...
I0624 10:33:40.850980    2565 start.go:240] waiting for startup goroutines ...
I0624 10:33:45.716452    2565 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: (11.966638302s)
I0624 10:33:45.718652    2565 out.go:177] üí°  Some dashboard features require the metrics-server addon. To enable all features please run:

	minikube addons enable metrics-server

I0624 10:33:45.716804    2565 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (11.902682697s)
I0624 10:33:45.716829    2565 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (11.228101391s)
I0624 10:33:45.736031    2565 out.go:177] üåü  Enabled addons: dashboard, storage-provisioner, default-storageclass
I0624 10:33:45.736741    2565 addons.go:505] duration metric: took 16.718259153s for enable addons: enabled=[dashboard storage-provisioner default-storageclass]
I0624 10:33:45.736795    2565 start.go:245] waiting for cluster config update ...
I0624 10:33:45.736810    2565 start.go:254] writing updated cluster config ...
I0624 10:33:45.737236    2565 ssh_runner.go:195] Run: rm -f paused
I0624 10:33:47.855060    2565 start.go:600] kubectl: 1.29.6, cluster: 1.30.0 (minor skew: 1)
I0624 10:33:47.856566    2565 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Jun 24 10:43:46 minikube dockerd[823]: time="2024-06-24T10:43:46.218440347Z" level=info msg="ignoring event" container=5f172e638ae39ad5db801446dd0cca22d9d3ab1d04b298632c6a2a27b714b8bc module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 24 10:44:16 minikube dockerd[823]: time="2024-06-24T10:44:16.226338283Z" level=info msg="ignoring event" container=a54f2ed074a71d13655269712ec685c15e283c94180ee69b33f5c9d431febade module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 24 10:44:16 minikube dockerd[823]: time="2024-06-24T10:44:16.358203159Z" level=info msg="ignoring event" container=6da41f47a7919532312062692d4ae2a4ebd735f3fa4b14b93c6ee8e548de7d39 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 24 10:44:16 minikube dockerd[823]: time="2024-06-24T10:44:16.408487991Z" level=info msg="ignoring event" container=76e7952fbfdb32f88be035818b27bce1b0b8f97bda8e91fde26219bd1fb018e7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 24 10:44:16 minikube dockerd[823]: time="2024-06-24T10:44:16.457198477Z" level=info msg="ignoring event" container=4e1ec391831b3bcfcd95427968ede9817a0f0dd5ec1d04e32947563002c038ad module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 24 10:44:16 minikube dockerd[823]: time="2024-06-24T10:44:16.988075287Z" level=info msg="ignoring event" container=a35a9519c71d1b9a78df590619391adbfe0e9ea47439d0739d4d88333a2e322e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 24 10:44:17 minikube dockerd[823]: time="2024-06-24T10:44:17.321562490Z" level=info msg="ignoring event" container=3ce45411a0347163bb07d3d95dd09ffdf1557c85119653bded6d4b79bac32e19 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 24 10:44:17 minikube dockerd[823]: time="2024-06-24T10:44:17.380539405Z" level=info msg="ignoring event" container=cd85e070c60620627c187a6831edb9b33b067e217bd7cd0fa6d80751eef46bf7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 24 10:44:17 minikube dockerd[823]: time="2024-06-24T10:44:17.477414879Z" level=info msg="ignoring event" container=3d74b9b949c2832174971f7e8f2ba0cb41dc43b000bebeb98c57f6507b5f1a22 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 24 10:44:52 minikube dockerd[823]: time="2024-06-24T10:44:52.785557410Z" level=warning msg="Published ports are discarded when using host network mode"
Jun 24 10:44:52 minikube dockerd[823]: time="2024-06-24T10:44:52.850638729Z" level=warning msg="Published ports are discarded when using host network mode"
Jun 24 10:44:53 minikube cri-dockerd[1037]: time="2024-06-24T10:44:53Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/794aadc902840e1b4ff2e0d69943c4816256ee3c5f637ff44ed670b692bf55a4/resolv.conf as [nameserver 192.168.1.6 search localdomain options edns0 trust-ad ndots:0]"
Jun 24 10:44:55 minikube cri-dockerd[1037]: time="2024-06-24T10:44:55Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/bad8d029c3471c5c6bf1cd6882c7c6d5be05d2273294f123a227d2e8f3fba0a8/resolv.conf as [nameserver 10.96.0.10 search krakend-monitoring.svc.cluster.local svc.cluster.local cluster.local localdomain options ndots:5]"
Jun 24 10:44:55 minikube cri-dockerd[1037]: time="2024-06-24T10:44:55Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c4ff78c28b29678bb87b965c1b1a6694f13dd205fc2f93496e3a24d90fae4cdd/resolv.conf as [nameserver 10.96.0.10 search krakend-monitoring.svc.cluster.local svc.cluster.local cluster.local localdomain options ndots:5]"
Jun 24 10:44:55 minikube cri-dockerd[1037]: time="2024-06-24T10:44:55Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/48451bad17a005ea32465b3d849538f83150c7ea79627a06446cd60e83b4040e/resolv.conf as [nameserver 10.96.0.10 search krakend-monitoring.svc.cluster.local svc.cluster.local cluster.local localdomain options ndots:5]"
Jun 24 10:44:55 minikube cri-dockerd[1037]: time="2024-06-24T10:44:55Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d8fca3ca79e7513ef93ef6b158694fac50da3ff3d7435bd56c7fb45d08d0f855/resolv.conf as [nameserver 10.96.0.10 search krakend-monitoring.svc.cluster.local svc.cluster.local cluster.local localdomain options ndots:5]"
Jun 24 10:45:46 minikube dockerd[823]: time="2024-06-24T10:45:46.456798698Z" level=info msg="ignoring event" container=5bd2ce32e5e3ff12f19e322eb9eeb163a7720e591bf1bb7974d5ba36a16bc2f7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 24 10:45:46 minikube dockerd[823]: time="2024-06-24T10:45:46.608274615Z" level=info msg="ignoring event" container=9fe30add3345d2df2a5cffe5ccff2799bbb9353753c62e204e75e2744c46b138 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 24 10:45:47 minikube dockerd[823]: time="2024-06-24T10:45:47.031855924Z" level=info msg="ignoring event" container=48451bad17a005ea32465b3d849538f83150c7ea79627a06446cd60e83b4040e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 24 10:46:03 minikube dockerd[823]: time="2024-06-24T10:46:03.843073945Z" level=info msg="ignoring event" container=ba969f1ef5ac90c8817e6927d408ff5c06fcd6fd84a6952498ed10ca88436019 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 24 10:46:19 minikube dockerd[823]: time="2024-06-24T10:46:19.586090139Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=27d38a5a3131c32f traceID=2ac64d8d683796759e592d91db6670e6
Jun 24 10:46:19 minikube dockerd[823]: time="2024-06-24T10:46:19.587966647Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jun 24 10:49:58 minikube cri-dockerd[1037]: time="2024-06-24T10:49:58Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6a2d77ad52faed4ce48fa44259e967b66aae5f94d342a5466f8b1772b195a2c3/resolv.conf as [nameserver 10.96.0.10 search krakend-monitoring.svc.cluster.local svc.cluster.local cluster.local localdomain options ndots:5]"
Jun 24 10:51:12 minikube dockerd[823]: time="2024-06-24T10:51:12.485400733Z" level=info msg="ignoring event" container=ae7021abd8632188593fb7f4b23f90062585c8b897d02ced03a1eb092db87bbf module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 24 10:51:26 minikube dockerd[823]: time="2024-06-24T10:51:26.581457600Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=3a8a1b71a086cdf5 traceID=b1798cc1a0c3d334a1f4b22467e63e9a
Jun 24 10:51:26 minikube dockerd[823]: time="2024-06-24T10:51:26.582007710Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jun 24 10:53:12 minikube cri-dockerd[1037]: time="2024-06-24T10:53:12Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/8f5ee413435bc6a9d6c73ec92733e8009602ec286e854c12e6e550b441bf4088/resolv.conf as [nameserver 10.96.0.10 search krakend-monitoring.svc.cluster.local svc.cluster.local cluster.local localdomain options ndots:5]"
Jun 24 10:53:21 minikube cri-dockerd[1037]: time="2024-06-24T10:53:21Z" level=info msg="Stop pulling image docker.io/library/busybox:1.31.1: Status: Downloaded newer image for busybox:1.31.1"
Jun 24 10:53:21 minikube dockerd[823]: time="2024-06-24T10:53:21.721493254Z" level=info msg="ignoring event" container=de73745a8454352641007ad0034b1dfd8fb4e13488e1a026b3756ffd5ba26132 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 24 10:56:18 minikube dockerd[823]: time="2024-06-24T10:56:18.648558004Z" level=info msg="ignoring event" container=ee28925bf9ee26f62406e04e06cf1a471b96695db408a60666656ba1769b10a9 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 24 10:56:29 minikube dockerd[823]: time="2024-06-24T10:56:29.297477762Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=228ed968eebbf443 traceID=fef317a2ef45916ec045c1692e616507
Jun 24 10:56:29 minikube dockerd[823]: time="2024-06-24T10:56:29.298183701Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jun 24 11:01:19 minikube dockerd[823]: time="2024-06-24T11:01:19.823241234Z" level=info msg="ignoring event" container=af890acbc63cf167982ae20e169a98bbca62405a2053ea4e0d9997e61aae1778 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 24 11:01:45 minikube dockerd[823]: time="2024-06-24T11:01:45.504340872Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=5e401ea4a85bb87f traceID=31e9bf3ab8d7dad19807dbfa0aa0b0d8
Jun 24 11:01:45 minikube dockerd[823]: time="2024-06-24T11:01:45.505292095Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jun 24 11:06:24 minikube dockerd[823]: time="2024-06-24T11:06:24.679812315Z" level=info msg="ignoring event" container=d86a93ae9631b8c7ff8159e19a71ee46528dfaae5b8d0bb0115b750a48be778a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 24 11:06:51 minikube dockerd[823]: time="2024-06-24T11:06:51.327536859Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=20b62253f475392b traceID=af941d8b26350b20752529e60657cc9c
Jun 24 11:06:51 minikube dockerd[823]: time="2024-06-24T11:06:51.328598658Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jun 24 11:11:27 minikube dockerd[823]: time="2024-06-24T11:11:27.837474165Z" level=info msg="ignoring event" container=23f9f49aec10bc3a31b4a78d9babc4a4dbdf8910e7698eff5eab3a68cc757dce module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 24 11:12:00 minikube dockerd[823]: time="2024-06-24T11:12:00.604290140Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=5ec23222a6b24986 traceID=169af114add3a5c3184e9e15e1eaa2c1
Jun 24 11:12:00 minikube dockerd[823]: time="2024-06-24T11:12:00.604552169Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jun 24 11:16:33 minikube dockerd[823]: time="2024-06-24T11:16:33.580763222Z" level=info msg="ignoring event" container=eff69f9ffd2ef3553499704bb7d93e6854087348ea9bbbf9127217934fbedad4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 24 11:16:36 minikube cri-dockerd[1037]: time="2024-06-24T11:16:36Z" level=error msg="error getting RW layer size for container ID '23f9f49aec10bc3a31b4a78d9babc4a4dbdf8910e7698eff5eab3a68cc757dce': Error response from daemon: No such container: 23f9f49aec10bc3a31b4a78d9babc4a4dbdf8910e7698eff5eab3a68cc757dce"
Jun 24 11:16:36 minikube cri-dockerd[1037]: time="2024-06-24T11:16:36Z" level=error msg="Set backoffDuration to : 1m0s for container ID '23f9f49aec10bc3a31b4a78d9babc4a4dbdf8910e7698eff5eab3a68cc757dce'"
Jun 24 11:17:11 minikube dockerd[823]: time="2024-06-24T11:17:11.684493180Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=59e9219b7458f20f traceID=d932036e4345db4ad06b79dbd26cd62d
Jun 24 11:17:11 minikube dockerd[823]: time="2024-06-24T11:17:11.684662676Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jun 24 11:21:40 minikube dockerd[823]: time="2024-06-24T11:21:40.560557135Z" level=info msg="ignoring event" container=4515435ea38edcb2c676604070aab20f868598afd856ba7e11f98bcd146921a3 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 24 11:22:22 minikube dockerd[823]: time="2024-06-24T11:22:22.790778318Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=0b5480bb93bf2cdf traceID=70a3b89cdf581fada83c9873574cd99f
Jun 24 11:22:22 minikube dockerd[823]: time="2024-06-24T11:22:22.791718281Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jun 24 11:26:49 minikube dockerd[823]: time="2024-06-24T11:26:49.541284917Z" level=info msg="ignoring event" container=ecc05b34807c0b78a17b60065292838e1a5524ccdd5a721baa3fd1b66d5d81b0 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 24 11:27:34 minikube dockerd[823]: time="2024-06-24T11:27:34.615009033Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=2964d501ddc2eeec traceID=33bb8c83d13f263fc5b972a20c267632
Jun 24 11:27:34 minikube dockerd[823]: time="2024-06-24T11:27:34.615778787Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jun 24 11:31:50 minikube dockerd[823]: time="2024-06-24T11:31:50.680330633Z" level=info msg="ignoring event" container=7b627f3d5c75c49b5c5e22feea91f8da7d500ca65835688db53a9db7c83c5d25 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 24 11:31:56 minikube cri-dockerd[1037]: time="2024-06-24T11:31:56Z" level=error msg="error getting RW layer size for container ID 'ecc05b34807c0b78a17b60065292838e1a5524ccdd5a721baa3fd1b66d5d81b0': Error response from daemon: No such container: ecc05b34807c0b78a17b60065292838e1a5524ccdd5a721baa3fd1b66d5d81b0"
Jun 24 11:31:56 minikube cri-dockerd[1037]: time="2024-06-24T11:31:56Z" level=error msg="Set backoffDuration to : 1m0s for container ID 'ecc05b34807c0b78a17b60065292838e1a5524ccdd5a721baa3fd1b66d5d81b0'"
Jun 24 11:32:37 minikube dockerd[823]: time="2024-06-24T11:32:37.715990807Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=44d07a95392a80b1 traceID=70902d65a62b414be369464a7bb2fb74
Jun 24 11:32:37 minikube dockerd[823]: time="2024-06-24T11:32:37.716179856Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jun 24 11:36:59 minikube dockerd[823]: time="2024-06-24T11:36:59.496547958Z" level=info msg="ignoring event" container=f2c39c9137f54a10aa0d721cb9f2cbe746c87a1800a677eba497e180d078cde8 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 24 11:37:47 minikube dockerd[823]: time="2024-06-24T11:37:47.494145571Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=d52a69a1097599ce traceID=d22511b18c1edb1563ceae895759b49e
Jun 24 11:37:47 minikube dockerd[823]: time="2024-06-24T11:37:47.494233364Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"


==> container status <==
CONTAINER           IMAGE                                                                                         CREATED             STATE               NAME                                 ATTEMPT             POD ID              POD
f2c39c9137f54       f3ae97789cbfd                                                                                 4 minutes ago       Exited              vaultwarden                          158                 d551002252c8d       vaultwarden-release-0
6ecf19c109367       f9095e2f0444d                                                                                 48 minutes ago      Running             grafana                              0                   8f5ee413435bc       my-grafana-754759c9d7-qqh44
de73745a84543       busybox@sha256:95cf004f559831017cdf4628aaf1bb30133677be8702a8c5f2994629f637a209               48 minutes ago      Exited              init-chown-data                      0                   8f5ee413435bc       my-grafana-754759c9d7-qqh44
c35ea5aae0a79       b74abbcc4eacb                                                                                 51 minutes ago      Running             prometheus-server                    0                   6a2d77ad52fae       prometheus-server-7448577b8c-5z2zh
8d8c888e31248       c7b6ac19f8751                                                                                 51 minutes ago      Running             prometheus-server-configmap-reload   0                   6a2d77ad52fae       prometheus-server-7448577b8c-5z2zh
c49c4efd7be94       11f11916f8cdf                                                                                 56 minutes ago      Running             alertmanager                         0                   d8fca3ca79e75       prometheus-alertmanager-0
fd54ed82276b9       b20e6a3825670                                                                                 56 minutes ago      Running             kube-state-metrics                   0                   c4ff78c28b296       prometheus-kube-state-metrics-67848d7455-dk489
5109a5389e821       75cb855e9cefb                                                                                 56 minutes ago      Running             pushgateway                          0                   bad8d029c3471       prometheus-prometheus-pushgateway-58cb869bcc-hj6tv
85a7a2775621e       0c6f6c1bdd47c                                                                                 56 minutes ago      Running             node-exporter                        0                   794aadc902840       prometheus-prometheus-node-exporter-9947k
7f4033a257f64       devopsfaith/krakend@sha256:4c678c224f6751224b4419ad88310797d5d6c2f896bfb2a69b43c6de697754e1   2 hours ago         Running             krakend                              0                   0286962a3f6da       krakend-796b7db89-nwltw
6764bcc8bb8cf       6e38f40d628db                                                                                 2 hours ago         Running             storage-provisioner                  12                  40a5f6cfc2870       storage-provisioner
349dc53545d56       fa49d631d5d2c                                                                                 2 hours ago         Running             sample-ms                            3                   0bbe79689a3bf       sample-ms-69b7b5f5f4-prbtd
a27f2793117d8       cbb01a7bd410d                                                                                 2 hours ago         Running             coredns                              7                   0d2a506ba38d0       coredns-7db6d8ff4d-5htm8
2332bfdfc066b       115053965e86b                                                                                 2 hours ago         Running             dashboard-metrics-scraper            6                   0e09bb1a5505c       dashboard-metrics-scraper-b5fc48f67-8cbtk
f6bc5c07759cd       cbb01a7bd410d                                                                                 2 hours ago         Running             coredns                              7                   59fc9838e4560       coredns-7db6d8ff4d-pvdxk
a27fe817d1ea7       ee54966f3891d                                                                                 2 hours ago         Running             controller                           3                   95eff74f76812       nginx-ingress-ingress-nginx-controller-657b44cd6f-dmv2s
fe95a9ff690da       07655ddf2eebe                                                                                 2 hours ago         Running             kubernetes-dashboard                 7                   ad2d26c8428c9       kubernetes-dashboard-779776cb65-qcf45
174dd82250811       2dccd97cabcf2                                                                                 2 hours ago         Running             psono                                3                   a2e8452801ce1       my-psono-78d5b657c5-4zrkg
3b22713b737e9       f35396d99bc53                                                                                 2 hours ago         Running             postgresql                           2                   d7e81b8df2afb       my-postgresql-0
c1963a358a7e6       6e38f40d628db                                                                                 2 hours ago         Exited              storage-provisioner                  11                  40a5f6cfc2870       storage-provisioner
9d378e1d69e37       a0bf559e280cf                                                                                 2 hours ago         Running             kube-proxy                           7                   91813b305fd8f       kube-proxy-48lpn
7566e71421ad2       c7aad43836fa5                                                                                 2 hours ago         Running             kube-controller-manager              7                   965cc114c73aa       kube-controller-manager-minikube
43b3e3ee0cfc5       3861cfcd7c04c                                                                                 2 hours ago         Running             etcd                                 7                   b9356a63bc008       etcd-minikube
1587896262b87       c42f13656d0b2                                                                                 2 hours ago         Running             kube-apiserver                       7                   5f860fa905ef7       kube-apiserver-minikube
94565ea858133       259c8277fcbbc                                                                                 2 hours ago         Running             kube-scheduler                       7                   890e2d5cbc217       kube-scheduler-minikube
da10ae625c898       2dccd97cabcf2                                                                                 2 days ago          Exited              psono                                2                   b13867c6abce2       my-psono-78d5b657c5-4zrkg
51531e7cae604       f35396d99bc53                                                                                 2 days ago          Exited              postgresql                           1                   3de45e96f0426       my-postgresql-0
6ef0ad7646f09       07655ddf2eebe                                                                                 2 days ago          Exited              kubernetes-dashboard                 6                   2fb30e80f1393       kubernetes-dashboard-779776cb65-qcf45
38fe3b7845e44       cbb01a7bd410d                                                                                 2 days ago          Exited              coredns                              6                   bf752aadd5806       coredns-7db6d8ff4d-5htm8
5158bf3db7a3b       115053965e86b                                                                                 2 days ago          Exited              dashboard-metrics-scraper            5                   3fad5fc341b3b       dashboard-metrics-scraper-b5fc48f67-8cbtk
0f7f3e55560d8       cbb01a7bd410d                                                                                 2 days ago          Exited              coredns                              6                   8d1aaec38ecd2       coredns-7db6d8ff4d-pvdxk
8da25f5c0cc1f       ee54966f3891d                                                                                 2 days ago          Exited              controller                           2                   c75a58e5e3269       nginx-ingress-ingress-nginx-controller-657b44cd6f-dmv2s
8b5d830cf3b11       fa49d631d5d2c                                                                                 2 days ago          Exited              sample-ms                            2                   4ea49066e93d4       sample-ms-69b7b5f5f4-prbtd
f9b77b5e31151       a0bf559e280cf                                                                                 2 days ago          Exited              kube-proxy                           6                   cddf2ee194957       kube-proxy-48lpn
f7d0a6ab14d50       259c8277fcbbc                                                                                 2 days ago          Exited              kube-scheduler                       6                   24072179df840       kube-scheduler-minikube
1e411355b1ff7       c42f13656d0b2                                                                                 2 days ago          Exited              kube-apiserver                       6                   5ab30d397d526       kube-apiserver-minikube
187eba541f0d2       c7aad43836fa5                                                                                 2 days ago          Exited              kube-controller-manager              6                   37fbf35ebee7a       kube-controller-manager-minikube
5d69e1fafab4e       3861cfcd7c04c                                                                                 2 days ago          Exited              etcd                                 6                   f298f4f459de0       etcd-minikube


==> coredns [0f7f3e55560d] <==
.:53
[INFO] plugin/reload: Running configuration SHA512 = 591cf328cccc12bc490481273e738df59329c62c0b729d94e8b61db9961c2fa5f046dd37f1cf888b953814040d180f52594972691cd6ff41be96639138a43908
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> coredns [38fe3b7845e4] <==
.:53
[INFO] plugin/reload: Running configuration SHA512 = 591cf328cccc12bc490481273e738df59329c62c0b729d94e8b61db9961c2fa5f046dd37f1cf888b953814040d180f52594972691cd6ff41be96639138a43908
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> coredns [a27f2793117d] <==
.:53
[INFO] plugin/reload: Running configuration SHA512 = 591cf328cccc12bc490481273e738df59329c62c0b729d94e8b61db9961c2fa5f046dd37f1cf888b953814040d180f52594972691cd6ff41be96639138a43908
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2


==> coredns [f6bc5c07759c] <==
.:53
[INFO] plugin/reload: Running configuration SHA512 = 591cf328cccc12bc490481273e738df59329c62c0b729d94e8b61db9961c2fa5f046dd37f1cf888b953814040d180f52594972691cd6ff41be96639138a43908
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=5883c09216182566a63dff4c326a6fc9ed2982ff
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_06_19T11_15_36_0700
                    minikube.k8s.io/version=v1.33.1
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Wed, 19 Jun 2024 10:15:33 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Mon, 24 Jun 2024 11:41:18 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Mon, 24 Jun 2024 11:39:39 +0000   Wed, 19 Jun 2024 10:15:31 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Mon, 24 Jun 2024 11:39:39 +0000   Wed, 19 Jun 2024 10:15:31 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Mon, 24 Jun 2024 11:39:39 +0000   Wed, 19 Jun 2024 10:15:31 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Mon, 24 Jun 2024 11:39:39 +0000   Wed, 19 Jun 2024 10:15:46 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                2
  ephemeral-storage:  50770432Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3963536Ki
  pods:               110
Allocatable:
  cpu:                2
  ephemeral-storage:  50770432Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3963536Ki
  pods:               110
System Info:
  Machine ID:                 061a5d7bafc74cf895f34c0fdf439758
  System UUID:                b685e149-a2c9-40b6-8aec-95b315980d8a
  Boot ID:                    969027d1-b7c5-4f1b-8c9f-2c44154e4bdc
  Kernel Version:             6.5.0-41-generic
  OS Image:                   Ubuntu 22.04.4 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://26.1.1
  Kubelet Version:            v1.30.0
  Kube-Proxy Version:         v1.30.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (23 in total)
  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---
  default                     my-postgresql-0                                            100m (5%!)(MISSING)     150m (7%!)(MISSING)   128Mi (3%!)(MISSING)       192Mi (4%!)(MISSING)     3d1h
  default                     my-psono-78d5b657c5-4zrkg                                  0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d
  default                     nginx-ingress-ingress-nginx-controller-657b44cd6f-dmv2s    100m (5%!)(MISSING)     0 (0%!)(MISSING)      90Mi (2%!)(MISSING)        0 (0%!)(MISSING)         3d23h
  default                     sample-microservice-78699ddbd9-xgbxl                       0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4d
  default                     sample-ms-69b7b5f5f4-prbtd                                 0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d23h
  krakend-monitoring          krakend-796b7db89-nwltw                                    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         111m
  krakend-monitoring          my-grafana-754759c9d7-qqh44                                0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         48m
  krakend-monitoring          prometheus-alertmanager-0                                  0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         56m
  krakend-monitoring          prometheus-kube-state-metrics-67848d7455-dk489             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         56m
  krakend-monitoring          prometheus-prometheus-node-exporter-9947k                  0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         56m
  krakend-monitoring          prometheus-prometheus-pushgateway-58cb869bcc-hj6tv         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         56m
  krakend-monitoring          prometheus-server-7448577b8c-5z2zh                         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         51m
  kube-system                 coredns-7db6d8ff4d-5htm8                                   100m (5%!)(MISSING)     0 (0%!)(MISSING)      70Mi (1%!)(MISSING)        170Mi (4%!)(MISSING)     5d1h
  kube-system                 coredns-7db6d8ff4d-pvdxk                                   100m (5%!)(MISSING)     0 (0%!)(MISSING)      70Mi (1%!)(MISSING)        170Mi (4%!)(MISSING)     5d1h
  kube-system                 etcd-minikube                                              100m (5%!)(MISSING)     0 (0%!)(MISSING)      100Mi (2%!)(MISSING)       0 (0%!)(MISSING)         5d1h
  kube-system                 kube-apiserver-minikube                                    250m (12%!)(MISSING)    0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         5d1h
  kube-system                 kube-controller-manager-minikube                           200m (10%!)(MISSING)    0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         5d1h
  kube-system                 kube-proxy-48lpn                                           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         5d1h
  kube-system                 kube-scheduler-minikube                                    100m (5%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         5d1h
  kube-system                 storage-provisioner                                        0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         5d
  kubernetes-dashboard        dashboard-metrics-scraper-b5fc48f67-8cbtk                  0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         5d
  kubernetes-dashboard        kubernetes-dashboard-779776cb65-qcf45                      0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         5d
  vaulwarden                  vaultwarden-release-0                                      0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4d23h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests     Limits
  --------           --------     ------
  cpu                1050m (52%!)(MISSING)  150m (7%!)(MISSING)
  memory             458Mi (11%!)(MISSING)  532Mi (13%!)(MISSING)
  ephemeral-storage  50Mi (0%!)(MISSING)    1Gi (2%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)       0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)       0 (0%!)(MISSING)
Events:              <none>


==> dmesg <==
[Jun24 09:31] core: CPUID marked event: 'cpu cycles' unavailable
[  +0.000000] core: CPUID marked event: 'instructions' unavailable
[  +0.000000] core: CPUID marked event: 'bus cycles' unavailable
[  +0.000000] core: CPUID marked event: 'cache references' unavailable
[  +0.000000] core: CPUID marked event: 'cache misses' unavailable
[  +0.000000] core: CPUID marked event: 'branch instructions' unavailable
[  +0.000000] core: CPUID marked event: 'branch misses' unavailable
[  +1.105128] device-mapper: core: CONFIG_IMA_DISABLE_HTABLE is disabled. Duplicate IMA measurements will not be recorded in the IMA log.
[  +0.000582] platform eisa.0: EISA: Cannot allocate resource for mainboard
[  +0.000002] platform eisa.0: Cannot allocate resource for EISA slot 1
[  +0.000001] platform eisa.0: Cannot allocate resource for EISA slot 2
[  +0.000001] platform eisa.0: Cannot allocate resource for EISA slot 3
[  +0.000002] platform eisa.0: Cannot allocate resource for EISA slot 4
[  +0.000001] platform eisa.0: Cannot allocate resource for EISA slot 5
[  +0.000001] platform eisa.0: Cannot allocate resource for EISA slot 6
[  +0.000001] platform eisa.0: Cannot allocate resource for EISA slot 7
[  +0.000002] platform eisa.0: Cannot allocate resource for EISA slot 8
[  +1.927947] piix4_smbus 0000:00:07.3: SMBus Host Controller not enabled!
[  +0.604816] sd 32:0:0:0: [sda] Assuming drive cache: write through
[  +0.465577] systemd[1]: memfd_create() called without MFD_EXEC or MFD_NOEXEC_SEAL set
[  +0.127149] block sda: the capability attribute has been deprecated.
[  +7.108032] sched: RT throttling activated
[  +2.147872] kauditd_printk_skb: 35 callbacks suppressed


==> etcd [43b3e3ee0cfc] <==
{"level":"info","ts":"2024-06-24T10:08:36.329117Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2696293872,"revision":39507,"compact-revision":39211}
{"level":"info","ts":"2024-06-24T10:13:36.280207Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":39837}
{"level":"info","ts":"2024-06-24T10:13:36.309338Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":39837,"took":"28.347676ms","hash":2163924122,"current-db-size-bytes":7290880,"current-db-size":"7.3 MB","current-db-size-in-use-bytes":2936832,"current-db-size-in-use":"2.9 MB"}
{"level":"info","ts":"2024-06-24T10:13:36.309388Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2163924122,"revision":39837,"compact-revision":39507}
{"level":"info","ts":"2024-06-24T10:18:36.287462Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":40122}
{"level":"info","ts":"2024-06-24T10:18:36.30507Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":40122,"took":"15.309027ms","hash":3402619253,"current-db-size-bytes":7290880,"current-db-size":"7.3 MB","current-db-size-in-use-bytes":2744320,"current-db-size-in-use":"2.7 MB"}
{"level":"info","ts":"2024-06-24T10:18:36.305142Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3402619253,"revision":40122,"compact-revision":39837}
{"level":"info","ts":"2024-06-24T10:23:36.296615Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":40408}
{"level":"info","ts":"2024-06-24T10:23:36.318964Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":40408,"took":"20.786569ms","hash":1667356922,"current-db-size-bytes":7290880,"current-db-size":"7.3 MB","current-db-size-in-use-bytes":2764800,"current-db-size-in-use":"2.8 MB"}
{"level":"info","ts":"2024-06-24T10:23:36.319152Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1667356922,"revision":40408,"compact-revision":40122}
{"level":"info","ts":"2024-06-24T10:28:36.302706Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":40693}
{"level":"info","ts":"2024-06-24T10:28:36.322626Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":40693,"took":"18.979727ms","hash":2868698634,"current-db-size-bytes":7290880,"current-db-size":"7.3 MB","current-db-size-in-use-bytes":2772992,"current-db-size-in-use":"2.8 MB"}
{"level":"info","ts":"2024-06-24T10:28:36.322798Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2868698634,"revision":40693,"compact-revision":40408}
{"level":"info","ts":"2024-06-24T10:30:46.153431Z","caller":"etcdserver/server.go:1401","msg":"triggering snapshot","local-member-id":"aec36adc501070cc","local-member-applied-index":50005,"local-member-snapshot-index":40004,"local-member-snapshot-count":10000}
{"level":"info","ts":"2024-06-24T10:30:46.174129Z","caller":"etcdserver/server.go:2420","msg":"saved snapshot","snapshot-index":50005}
{"level":"info","ts":"2024-06-24T10:30:46.176388Z","caller":"etcdserver/server.go:2450","msg":"compacted Raft logs","compact-index":45005}
{"level":"info","ts":"2024-06-24T10:33:36.309517Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":40977}
{"level":"info","ts":"2024-06-24T10:33:36.413398Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":40977,"took":"103.465878ms","hash":3721983575,"current-db-size-bytes":7290880,"current-db-size":"7.3 MB","current-db-size-in-use-bytes":2760704,"current-db-size-in-use":"2.8 MB"}
{"level":"info","ts":"2024-06-24T10:33:36.413485Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3721983575,"revision":40977,"compact-revision":40693}
{"level":"info","ts":"2024-06-24T10:38:36.331382Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":41264}
{"level":"info","ts":"2024-06-24T10:38:36.355029Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":41264,"took":"22.758786ms","hash":3568873418,"current-db-size-bytes":7290880,"current-db-size":"7.3 MB","current-db-size-in-use-bytes":3174400,"current-db-size-in-use":"3.2 MB"}
{"level":"info","ts":"2024-06-24T10:38:36.355098Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3568873418,"revision":41264,"compact-revision":40977}
{"level":"info","ts":"2024-06-24T10:43:36.339612Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":41628}
{"level":"info","ts":"2024-06-24T10:43:36.382656Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":41628,"took":"41.841104ms","hash":2359392102,"current-db-size-bytes":7290880,"current-db-size":"7.3 MB","current-db-size-in-use-bytes":3633152,"current-db-size-in-use":"3.6 MB"}
{"level":"info","ts":"2024-06-24T10:43:36.382787Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2359392102,"revision":41628,"compact-revision":41264}
{"level":"info","ts":"2024-06-24T10:48:36.345344Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":41965}
{"level":"info","ts":"2024-06-24T10:48:36.430923Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":41965,"took":"84.773891ms","hash":1830707434,"current-db-size-bytes":9117696,"current-db-size":"9.1 MB","current-db-size-in-use-bytes":6774784,"current-db-size-in-use":"6.8 MB"}
{"level":"info","ts":"2024-06-24T10:48:36.431273Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1830707434,"revision":41965,"compact-revision":41628}
{"level":"info","ts":"2024-06-24T10:53:36.35039Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":42577}
{"level":"warn","ts":"2024-06-24T10:53:36.526131Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"114.079224ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128030073253821344 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:42936 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >>","response":"size:18"}
{"level":"info","ts":"2024-06-24T10:53:36.544703Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":42577,"took":"192.340111ms","hash":3902652856,"current-db-size-bytes":9244672,"current-db-size":"9.2 MB","current-db-size-in-use-bytes":3072000,"current-db-size-in-use":"3.1 MB"}
{"level":"info","ts":"2024-06-24T10:53:36.544847Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3902652856,"revision":42577,"compact-revision":41965}
{"level":"info","ts":"2024-06-24T10:53:36.549503Z","caller":"traceutil/trace.go:171","msg":"trace[995019773] transaction","detail":"{read_only:false; response_revision:42939; number_of_response:1; }","duration":"140.511544ms","start":"2024-06-24T10:53:36.392252Z","end":"2024-06-24T10:53:36.532763Z","steps":["trace[995019773] 'get key's previous created_revision and leaseID' {req_type:put; key:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; req_size:1090; } (duration: 113.24485ms)"],"step_count":1}
{"level":"info","ts":"2024-06-24T10:58:36.356845Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":42938}
{"level":"info","ts":"2024-06-24T10:58:36.441397Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":42938,"took":"83.953659ms","hash":2241853769,"current-db-size-bytes":9244672,"current-db-size":"9.2 MB","current-db-size-in-use-bytes":3571712,"current-db-size-in-use":"3.6 MB"}
{"level":"info","ts":"2024-06-24T10:58:36.441467Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2241853769,"revision":42938,"compact-revision":42577}
{"level":"info","ts":"2024-06-24T11:03:36.362178Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":43225}
{"level":"info","ts":"2024-06-24T11:03:36.36907Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":43225,"took":"6.497913ms","hash":792134315,"current-db-size-bytes":9244672,"current-db-size":"9.2 MB","current-db-size-in-use-bytes":2957312,"current-db-size-in-use":"3.0 MB"}
{"level":"info","ts":"2024-06-24T11:03:36.369122Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":792134315,"revision":43225,"compact-revision":42938}
{"level":"info","ts":"2024-06-24T11:08:36.37281Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":43509}
{"level":"info","ts":"2024-06-24T11:08:36.37882Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":43509,"took":"5.233457ms","hash":1391473747,"current-db-size-bytes":9244672,"current-db-size":"9.2 MB","current-db-size-in-use-bytes":2957312,"current-db-size-in-use":"3.0 MB"}
{"level":"info","ts":"2024-06-24T11:08:36.378912Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1391473747,"revision":43509,"compact-revision":43225}
{"level":"info","ts":"2024-06-24T11:13:36.383421Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":43796}
{"level":"info","ts":"2024-06-24T11:13:36.393235Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":43796,"took":"8.359244ms","hash":1943305017,"current-db-size-bytes":9244672,"current-db-size":"9.2 MB","current-db-size-in-use-bytes":2957312,"current-db-size-in-use":"3.0 MB"}
{"level":"info","ts":"2024-06-24T11:13:36.39362Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1943305017,"revision":43796,"compact-revision":43509}
{"level":"info","ts":"2024-06-24T11:18:36.39029Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":44081}
{"level":"info","ts":"2024-06-24T11:18:36.395996Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":44081,"took":"5.02083ms","hash":1523039229,"current-db-size-bytes":9244672,"current-db-size":"9.2 MB","current-db-size-in-use-bytes":2920448,"current-db-size-in-use":"2.9 MB"}
{"level":"info","ts":"2024-06-24T11:18:36.396138Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1523039229,"revision":44081,"compact-revision":43796}
{"level":"info","ts":"2024-06-24T11:23:36.398026Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":44366}
{"level":"info","ts":"2024-06-24T11:23:36.403508Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":44366,"took":"4.160171ms","hash":2432289509,"current-db-size-bytes":9244672,"current-db-size":"9.2 MB","current-db-size-in-use-bytes":2912256,"current-db-size-in-use":"2.9 MB"}
{"level":"info","ts":"2024-06-24T11:23:36.403619Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2432289509,"revision":44366,"compact-revision":44081}
{"level":"info","ts":"2024-06-24T11:28:36.40483Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":44652}
{"level":"info","ts":"2024-06-24T11:28:36.410163Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":44652,"took":"4.182348ms","hash":1061782709,"current-db-size-bytes":9244672,"current-db-size":"9.2 MB","current-db-size-in-use-bytes":2887680,"current-db-size-in-use":"2.9 MB"}
{"level":"info","ts":"2024-06-24T11:28:36.410541Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1061782709,"revision":44652,"compact-revision":44366}
{"level":"info","ts":"2024-06-24T11:33:36.41385Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":44938}
{"level":"info","ts":"2024-06-24T11:33:36.426483Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":44938,"took":"11.442419ms","hash":3661504353,"current-db-size-bytes":9244672,"current-db-size":"9.2 MB","current-db-size-in-use-bytes":2916352,"current-db-size-in-use":"2.9 MB"}
{"level":"info","ts":"2024-06-24T11:33:36.426686Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3661504353,"revision":44938,"compact-revision":44652}
{"level":"info","ts":"2024-06-24T11:38:36.419527Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":45223}
{"level":"info","ts":"2024-06-24T11:38:36.423054Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":45223,"took":"3.16385ms","hash":2273312926,"current-db-size-bytes":9244672,"current-db-size":"9.2 MB","current-db-size-in-use-bytes":2936832,"current-db-size-in-use":"2.9 MB"}
{"level":"info","ts":"2024-06-24T11:38:36.423274Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2273312926,"revision":45223,"compact-revision":44938}


==> etcd [5d69e1fafab4] <==
{"level":"info","ts":"2024-06-21T14:33:08.92663Z","caller":"etcdserver/server.go:524","msg":"recovered v3 backend from snapshot","backend-size-bytes":6545408,"backend-size":"6.5 MB","backend-size-in-use-bytes":2174976,"backend-size-in-use":"2.2 MB"}
{"level":"info","ts":"2024-06-21T14:33:09.443372Z","caller":"etcdserver/raft.go:530","msg":"restarting local member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","commit-index":42894}
{"level":"info","ts":"2024-06-21T14:33:09.44466Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2024-06-21T14:33:09.444742Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 7"}
{"level":"info","ts":"2024-06-21T14:33:09.444781Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [aec36adc501070cc], term: 7, commit: 42894, applied: 40004, lastindex: 42894, lastterm: 7]"}
{"level":"info","ts":"2024-06-21T14:33:09.444938Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2024-06-21T14:33:09.445324Z","caller":"membership/cluster.go:278","msg":"recovered/added member from store","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","recovered-remote-peer-id":"aec36adc501070cc","recovered-remote-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-06-21T14:33:09.44535Z","caller":"membership/cluster.go:287","msg":"set cluster version from store","cluster-version":"3.5"}
{"level":"warn","ts":"2024-06-21T14:33:09.445844Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2024-06-21T14:33:09.446363Z","caller":"mvcc/kvstore.go:341","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":34568}
{"level":"info","ts":"2024-06-21T14:33:09.453133Z","caller":"mvcc/kvstore.go:407","msg":"kvstore restored","current-rev":35076}
{"level":"info","ts":"2024-06-21T14:33:09.454289Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2024-06-21T14:33:09.456125Z","caller":"etcdserver/corrupt.go:96","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2024-06-21T14:33:09.45682Z","caller":"etcdserver/corrupt.go:177","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-06-21T14:33:09.45692Z","caller":"etcdserver/server.go:851","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.12","cluster-id":"fa54960ea34d58be","cluster-version":"3.5"}
{"level":"info","ts":"2024-06-21T14:33:09.458327Z","caller":"etcdserver/server.go:744","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2024-06-21T14:33:09.458823Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2024-06-21T14:33:09.458921Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2024-06-21T14:33:09.458958Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2024-06-21T14:33:09.461538Z","caller":"embed/etcd.go:726","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-06-21T14:33:09.476183Z","caller":"embed/etcd.go:277","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2024-06-21T14:33:09.476564Z","caller":"embed/etcd.go:857","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2024-06-21T14:33:09.47014Z","caller":"embed/etcd.go:597","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-06-21T14:33:09.476737Z","caller":"embed/etcd.go:569","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-06-21T14:33:10.246496Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 7"}
{"level":"info","ts":"2024-06-21T14:33:10.246606Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 7"}
{"level":"info","ts":"2024-06-21T14:33:10.246809Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 7"}
{"level":"info","ts":"2024-06-21T14:33:10.246881Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 8"}
{"level":"info","ts":"2024-06-21T14:33:10.246906Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 8"}
{"level":"info","ts":"2024-06-21T14:33:10.246929Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 8"}
{"level":"info","ts":"2024-06-21T14:33:10.246949Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 8"}
{"level":"info","ts":"2024-06-21T14:33:10.249496Z","caller":"etcdserver/server.go:2068","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2024-06-21T14:33:10.249786Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-06-21T14:33:10.250118Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-06-21T14:33:10.252625Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-06-21T14:33:10.252719Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-06-21T14:33:10.257262Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2024-06-21T14:33:10.257616Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2024-06-21T14:43:10.319194Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":35787}
{"level":"info","ts":"2024-06-21T14:43:10.441297Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":35787,"took":"118.599526ms","hash":3071061917,"current-db-size-bytes":6893568,"current-db-size":"6.9 MB","current-db-size-in-use-bytes":4984832,"current-db-size-in-use":"5.0 MB"}
{"level":"info","ts":"2024-06-21T14:43:10.441448Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3071061917,"revision":35787,"compact-revision":34568}
{"level":"info","ts":"2024-06-21T14:48:10.32901Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":36101}
{"level":"info","ts":"2024-06-21T14:48:10.334427Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":36101,"took":"4.285031ms","hash":4006035897,"current-db-size-bytes":6893568,"current-db-size":"6.9 MB","current-db-size-in-use-bytes":4063232,"current-db-size-in-use":"4.1 MB"}
{"level":"info","ts":"2024-06-21T14:48:10.334919Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4006035897,"revision":36101,"compact-revision":35787}
{"level":"info","ts":"2024-06-21T14:53:10.357349Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":36449}
{"level":"info","ts":"2024-06-21T14:53:10.378024Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":36449,"took":"19.467317ms","hash":3868143214,"current-db-size-bytes":6893568,"current-db-size":"6.9 MB","current-db-size-in-use-bytes":4927488,"current-db-size-in-use":"4.9 MB"}
{"level":"info","ts":"2024-06-21T14:53:10.378117Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3868143214,"revision":36449,"compact-revision":36101}
{"level":"info","ts":"2024-06-21T14:58:10.364701Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":36793}
{"level":"info","ts":"2024-06-21T14:58:10.375963Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":36793,"took":"9.670157ms","hash":32329176,"current-db-size-bytes":6893568,"current-db-size":"6.9 MB","current-db-size-in-use-bytes":4509696,"current-db-size-in-use":"4.5 MB"}
{"level":"info","ts":"2024-06-21T14:58:10.376103Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":32329176,"revision":36793,"compact-revision":36449}
{"level":"info","ts":"2024-06-21T15:01:00.483108Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2024-06-21T15:01:00.486256Z","caller":"embed/etcd.go:375","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"warn","ts":"2024-06-21T15:01:00.580723Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-06-21T15:01:00.581455Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-06-21T15:01:01.100212Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2024-06-21T15:01:01.100265Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"info","ts":"2024-06-21T15:01:01.102008Z","caller":"etcdserver/server.go:1471","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-06-21T15:01:01.291123Z","caller":"embed/etcd.go:579","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-06-21T15:01:01.300585Z","caller":"embed/etcd.go:584","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-06-21T15:01:01.300619Z","caller":"embed/etcd.go:377","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}


==> kernel <==
 11:41:25 up  2:10,  0 users,  load average: 2.16, 2.42, 2.50
Linux minikube 6.5.0-41-generic #41~22.04.2-Ubuntu SMP PREEMPT_DYNAMIC Mon Jun  3 11:32:55 UTC 2 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.4 LTS"


==> kube-apiserver [1587896262b8] <==
I0624 09:33:38.606146       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0624 09:33:38.606265       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0624 09:33:38.606289       1 crd_finalizer.go:266] Starting CRDFinalizer
I0624 09:33:38.606784       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I0624 09:33:38.606827       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I0624 09:33:38.607354       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0624 09:33:38.607498       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0624 09:33:38.584837       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0624 09:33:38.885692       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0624 09:33:38.896440       1 shared_informer.go:320] Caches are synced for node_authorizer
I0624 09:33:38.896497       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0624 09:33:38.896728       1 cache.go:39] Caches are synced for AvailableConditionController controller
I0624 09:33:38.897206       1 shared_informer.go:320] Caches are synced for configmaps
I0624 09:33:38.907909       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0624 09:33:38.907974       1 apf_controller.go:379] Running API Priority and Fairness config worker
I0624 09:33:38.907982       1 apf_controller.go:382] Running API Priority and Fairness periodic rebalancing process
I0624 09:33:38.908166       1 aggregator.go:165] initial CRD sync complete...
I0624 09:33:38.908175       1 autoregister_controller.go:141] Starting autoregister controller
I0624 09:33:38.908181       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0624 09:33:38.908186       1 cache.go:39] Caches are synced for autoregister controller
I0624 09:33:38.925041       1 handler_discovery.go:447] Starting ResourceDiscoveryManager
I0624 09:33:38.925137       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0624 09:33:38.926094       1 policy_source.go:224] refreshing policies
E0624 09:33:38.950977       1 controller.go:97] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
I0624 09:33:38.980288       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
E0624 09:33:39.008182       1 controller.go:195] "Failed to update lease" err="Operation cannot be fulfilled on leases.coordination.k8s.io \"apiserver-eqt674mfxb4j56mrjjkoe7b7ii\": StorageError: invalid object, Code: 4, Key: /registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii, ResourceVersion: 0, AdditionalErrorMsg: Precondition failed: UID in precondition: 25766619-4979-4c27-906d-a7d55c7d26a7, UID in object meta: "
I0624 09:33:39.706480       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
E0624 09:33:50.648444       1 storage.go:475] Address {10.244.0.87  0xc00b0c3740 0xc00b0d9c70} isn't valid (pod ip(s) doesn't match endpoint ip, skipping: [] vs 10.244.0.87 (kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67-8cbtk))
E0624 09:33:50.648551       1 storage.go:485] Failed to find a valid address, skipping subset: &{[{10.244.0.87  0xc00b0c3740 0xc00b0d9c70}] [] [{ 8000 TCP <nil>}]}
I0624 09:33:51.792950       1 controller.go:615] quota admission added evaluator for: endpoints
I0624 09:33:51.816245       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0624 09:33:52.655351       1 trace.go:236] Trace[1012354928]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:41f6e12b-79fd-4279-94f2-610c85eec679,client:192.168.49.2,api-group:apps,api-version:v1,name:my-postgresql,subresource:status,namespace:default,protocol:HTTP/2.0,resource:statefulsets,scope:resource,url:/apis/apps/v1/namespaces/default/statefulsets/my-postgresql/status,user-agent:kube-controller-manager/v1.30.0 (linux/amd64) kubernetes/7c48c2b/system:serviceaccount:kube-system:statefulset-controller,verb:PUT (24-Jun-2024 09:33:52.102) (total time: 553ms):
Trace[1012354928]: ---"limitedReadBody succeeded" len:2685 150ms (09:33:52.252)
Trace[1012354928]: ---"Writing http response done" 40ms (09:33:52.655)
Trace[1012354928]: [553.067255ms] [553.067255ms] END
I0624 09:33:52.655829       1 trace.go:236] Trace[43818311]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:4472b80c-75da-4f42-94bd-88cea5f160e9,client:192.168.49.2,api-group:apps,api-version:v1,name:prometheus-alertmanager,subresource:status,namespace:krakend-monitoring,protocol:HTTP/2.0,resource:statefulsets,scope:resource,url:/apis/apps/v1/namespaces/krakend-monitoring/statefulsets/prometheus-alertmanager/status,user-agent:kube-controller-manager/v1.30.0 (linux/amd64) kubernetes/7c48c2b/system:serviceaccount:kube-system:statefulset-controller,verb:PUT (24-Jun-2024 09:33:52.119) (total time: 535ms):
Trace[43818311]: ---"limitedReadBody succeeded" len:1564 154ms (09:33:52.274)
Trace[43818311]: [535.803047ms] [535.803047ms] END
I0624 09:41:08.139616       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I0624 09:41:08.173506       1 alloc.go:330] "allocated clusterIPs" service="krakend-monitoring/krakend" clusterIPs={"IPv4":"10.106.227.155"}
I0624 09:41:08.187397       1 controller.go:615] quota admission added evaluator for: deployments.apps
I0624 09:41:08.196338       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0624 09:43:51.237341       1 alloc.go:330] "allocated clusterIPs" service="krakend-monitoring/krakend" clusterIPs={"IPv4":"10.103.139.201"}
I0624 09:50:23.466648       1 alloc.go:330] "allocated clusterIPs" service="krakend-monitoring/krakend" clusterIPs={"IPv4":"10.99.254.197"}
I0624 09:56:43.424367       1 alloc.go:330] "allocated clusterIPs" service="krakend-monitoring/krakend-ext" clusterIPs={"IPv4":"10.111.9.62"}
I0624 10:00:36.648326       1 alloc.go:330] "allocated clusterIPs" service="krakend-monitoring/krakend-ext" clusterIPs={"IPv4":"10.106.97.37"}
I0624 10:05:24.284741       1 controller.go:615] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0624 10:05:24.290652       1 controller.go:615] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0624 10:05:24.326192       1 alloc.go:330] "allocated clusterIPs" service="krakend-monitoring/grafana" clusterIPs={"IPv4":"10.105.148.217"}
I0624 10:07:19.581356       1 alloc.go:330] "allocated clusterIPs" service="krakend-monitoring/grafana-ext" clusterIPs={"IPv4":"10.105.94.178"}
I0624 10:38:18.149316       1 controller.go:615] quota admission added evaluator for: daemonsets.apps
I0624 10:38:18.421047       1 controller.go:615] quota admission added evaluator for: statefulsets.apps
E0624 10:44:16.096314       1 watch.go:250] write tcp 192.168.49.2:8443->10.244.0.109:59282: write: connection reset by peer
I0624 10:44:51.433537       1 alloc.go:330] "allocated clusterIPs" service="krakend-monitoring/prometheus-prometheus-node-exporter" clusterIPs={"IPv4":"10.108.69.8"}
I0624 10:44:51.485406       1 alloc.go:330] "allocated clusterIPs" service="krakend-monitoring/prometheus-prometheus-pushgateway" clusterIPs={"IPv4":"10.97.37.101"}
I0624 10:44:51.507484       1 alloc.go:330] "allocated clusterIPs" service="krakend-monitoring/prometheus-kube-state-metrics" clusterIPs={"IPv4":"10.103.52.154"}
I0624 10:44:51.577061       1 alloc.go:330] "allocated clusterIPs" service="krakend-monitoring/prometheus-alertmanager" clusterIPs={"IPv4":"10.101.171.85"}
I0624 10:44:51.608055       1 alloc.go:330] "allocated clusterIPs" service="krakend-monitoring/prometheus-server" clusterIPs={"IPv4":"10.96.113.160"}
I0624 10:44:51.824925       1 controller.go:615] quota admission added evaluator for: controllerrevisions.apps
I0624 10:53:11.506114       1 alloc.go:330] "allocated clusterIPs" service="krakend-monitoring/my-grafana" clusterIPs={"IPv4":"10.107.219.54"}


==> kube-apiserver [1e411355b1ff] <==
I0621 15:01:00.758100       1 controller.go:176] quota evaluator worker shutdown
I0621 15:01:00.758109       1 controller.go:176] quota evaluator worker shutdown
I0621 15:01:00.758115       1 controller.go:176] quota evaluator worker shutdown
E0621 15:01:00.786047       1 watcher.go:342] watch chan error: rpc error: code = Unknown desc = malformed header: missing HTTP content-type
W0621 15:01:00.789302       1 logging.go:59] [core] [Channel #142 SubChannel #143] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 15:01:00.800990       1 logging.go:59] [core] [Channel #100 SubChannel #101] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 15:01:00.801064       1 logging.go:59] [core] [Channel #118 SubChannel #119] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 15:01:00.801109       1 logging.go:59] [core] [Channel #121 SubChannel #122] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 15:01:00.801149       1 logging.go:59] [core] [Channel #154 SubChannel #155] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 15:01:00.801197       1 logging.go:59] [core] [Channel #37 SubChannel #38] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 15:01:00.801236       1 logging.go:59] [core] [Channel #61 SubChannel #62] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 15:01:00.801274       1 logging.go:59] [core] [Channel #109 SubChannel #110] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 15:01:00.801310       1 logging.go:59] [core] [Channel #157 SubChannel #158] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 15:01:00.801349       1 logging.go:59] [core] [Channel #25 SubChannel #26] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 15:01:00.801421       1 logging.go:59] [core] [Channel #79 SubChannel #80] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 15:01:00.801468       1 logging.go:59] [core] [Channel #64 SubChannel #65] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 15:01:00.801521       1 logging.go:59] [core] [Channel #97 SubChannel #98] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 15:01:00.801557       1 logging.go:59] [core] [Channel #19 SubChannel #20] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 15:01:00.801595       1 logging.go:59] [core] [Channel #160 SubChannel #161] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 15:01:00.801645       1 logging.go:59] [core] [Channel #34 SubChannel #35] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 15:01:00.801683       1 logging.go:59] [core] [Channel #139 SubChannel #140] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 15:01:00.801724       1 logging.go:59] [core] [Channel #58 SubChannel #59] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 15:01:00.801760       1 logging.go:59] [core] [Channel #67 SubChannel #68] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 15:01:00.801806       1 logging.go:59] [core] [Channel #106 SubChannel #107] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 15:01:00.801859       1 logging.go:59] [core] [Channel #166 SubChannel #167] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 15:01:00.801924       1 logging.go:59] [core] [Channel #172 SubChannel #173] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 15:01:00.802013       1 logging.go:59] [core] [Channel #115 SubChannel #116] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 15:01:00.802057       1 logging.go:59] [core] [Channel #124 SubChannel #125] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 15:01:00.802095       1 logging.go:59] [core] [Channel #178 SubChannel #179] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 15:01:00.802132       1 logging.go:59] [core] [Channel #91 SubChannel #92] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 15:01:00.802171       1 logging.go:59] [core] [Channel #175 SubChannel #176] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 15:01:00.802212       1 logging.go:59] [core] [Channel #22 SubChannel #23] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 15:01:00.802254       1 logging.go:59] [core] [Channel #127 SubChannel #128] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 15:01:00.802304       1 logging.go:59] [core] [Channel #151 SubChannel #152] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 15:01:00.802347       1 logging.go:59] [core] [Channel #169 SubChannel #170] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 15:01:00.809974       1 logging.go:59] [core] [Channel #136 SubChannel #137] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 15:01:00.810132       1 logging.go:59] [core] [Channel #16 SubChannel #17] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 15:01:00.810186       1 logging.go:59] [core] [Channel #76 SubChannel #77] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 15:01:00.810233       1 logging.go:59] [core] [Channel #130 SubChannel #131] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 15:01:00.810292       1 logging.go:59] [core] [Channel #82 SubChannel #83] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 15:01:00.810354       1 logging.go:59] [core] [Channel #49 SubChannel #50] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 15:01:01.000143       1 logging.go:59] [core] [Channel #85 SubChannel #86] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 15:01:01.000304       1 logging.go:59] [core] [Channel #43 SubChannel #44] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 15:01:01.000371       1 logging.go:59] [core] [Channel #181 SubChannel #182] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 15:01:01.000417       1 logging.go:59] [core] [Channel #28 SubChannel #29] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 15:01:01.000463       1 logging.go:59] [core] [Channel #52 SubChannel #53] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 15:01:01.000506       1 logging.go:59] [core] [Channel #70 SubChannel #71] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 15:01:01.000837       1 logging.go:59] [core] [Channel #88 SubChannel #89] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 15:01:01.000894       1 logging.go:59] [core] [Channel #55 SubChannel #56] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 15:01:01.000937       1 logging.go:59] [core] [Channel #112 SubChannel #113] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 15:01:01.001008       1 logging.go:59] [core] [Channel #46 SubChannel #47] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 15:01:01.001050       1 logging.go:59] [core] [Channel #103 SubChannel #104] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 15:01:01.001092       1 logging.go:59] [core] [Channel #133 SubChannel #134] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 15:01:01.001137       1 logging.go:59] [core] [Channel #163 SubChannel #164] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 15:01:01.001177       1 logging.go:59] [core] [Channel #31 SubChannel #32] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 15:01:01.001227       1 logging.go:59] [core] [Channel #5 SubChannel #6] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 15:01:01.017623       1 logging.go:59] [core] [Channel #145 SubChannel #146] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 15:01:01.017775       1 logging.go:59] [core] [Channel #94 SubChannel #95] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 15:01:01.017829       1 logging.go:59] [core] [Channel #148 SubChannel #149] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 15:01:01.017876       1 logging.go:59] [core] [Channel #73 SubChannel #74] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"


==> kube-controller-manager [187eba541f0d] <==
I0621 14:33:33.501181       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/my-psono-78d5b657c5" duration="73.944¬µs"
I0621 14:33:34.334073       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-ingress-ingress-nginx-controller-657b44cd6f" duration="22.291061ms"
I0621 14:33:34.334176       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-ingress-ingress-nginx-controller-657b44cd6f" duration="59.911¬µs"
I0621 14:33:34.554373       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/my-psono-78d5b657c5" duration="57.178¬µs"
I0621 14:33:38.784010       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/sample-microservice-78699ddbd9" duration="110.201¬µs"
I0621 14:33:48.043759       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/my-psono-78d5b657c5" duration="86.559¬µs"
I0621 14:33:51.796672       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/sample-microservice-78699ddbd9" duration="48.595¬µs"
I0621 14:33:54.067297       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/my-psono-78d5b657c5" duration="39.218403ms"
I0621 14:33:54.074298       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/my-psono-78d5b657c5" duration="271.79¬µs"
I0621 14:34:05.818808       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/sample-microservice-78699ddbd9" duration="48.251¬µs"
I0621 14:34:22.785741       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/sample-microservice-78699ddbd9" duration="56.495¬µs"
I0621 14:34:35.336738       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/prometheus-server-5787759b8c" duration="161.687026ms"
I0621 14:34:35.452149       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/prometheus-prometheus-pushgateway-58cb869bcc" duration="176.321628ms"
I0621 14:34:35.452925       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/prometheus-server-5787759b8c" duration="115.690327ms"
I0621 14:34:35.453776       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/prometheus-kube-state-metrics-67848d7455" duration="199.430474ms"
I0621 14:34:35.631053       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/prometheus-prometheus-pushgateway-58cb869bcc" duration="178.72678ms"
I0621 14:34:35.631318       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/prometheus-prometheus-pushgateway-58cb869bcc" duration="206.988¬µs"
I0621 14:34:35.653968       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/prometheus-server-5787759b8c" duration="200.687101ms"
I0621 14:34:35.654379       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/prometheus-server-5787759b8c" duration="144.13¬µs"
I0621 14:34:35.658262       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/prometheus-kube-state-metrics-67848d7455" duration="204.36556ms"
I0621 14:34:35.663760       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/prometheus-kube-state-metrics-67848d7455" duration="5.263296ms"
I0621 14:34:35.668711       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/prometheus-kube-state-metrics-67848d7455" duration="176.819¬µs"
I0621 14:34:35.815053       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/sample-microservice-78699ddbd9" duration="111.034¬µs"
I0621 14:36:21.419696       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/prometheus-kube-state-metrics-67848d7455" duration="121.733¬µs"
I0621 14:36:26.048172       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/prometheus-kube-state-metrics-67848d7455" duration="19.787311ms"
I0621 14:36:26.050212       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/prometheus-kube-state-metrics-67848d7455" duration="1.040677ms"
I0621 14:36:28.653916       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/prometheus-prometheus-pushgateway-58cb869bcc" duration="59.767¬µs"
I0621 14:36:46.021944       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/prometheus-prometheus-pushgateway-58cb869bcc" duration="16.166313ms"
I0621 14:36:46.022056       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/prometheus-prometheus-pushgateway-58cb869bcc" duration="63.2¬µs"
I0621 14:37:06.787107       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/sample-microservice-78699ddbd9" duration="69.411¬µs"
I0621 14:37:20.770596       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/sample-microservice-78699ddbd9" duration="85.844¬µs"
I0621 14:37:28.179940       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/prometheus-server-5787759b8c" duration="173.433¬µs"
I0621 14:38:01.540012       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/prometheus-server-5787759b8c" duration="37.563997ms"
I0621 14:38:01.541110       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/prometheus-server-5787759b8c" duration="96.93¬µs"
I0621 14:38:39.778909       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/sample-microservice-78699ddbd9" duration="30.19¬µs"
I0621 14:38:53.814363       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/sample-microservice-78699ddbd9" duration="63.204¬µs"
I0621 14:41:24.771303       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/sample-microservice-78699ddbd9" duration="140.052¬µs"
I0621 14:41:36.768948       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/sample-microservice-78699ddbd9" duration="59.964¬µs"
I0621 14:42:43.443811       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/grafana-74cd95df6f" duration="131.274633ms"
I0621 14:42:43.538145       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/grafana-74cd95df6f" duration="94.267343ms"
I0621 14:42:43.558913       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/grafana-74cd95df6f" duration="71.51¬µs"
I0621 14:42:43.577343       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/grafana-74cd95df6f" duration="115.214¬µs"
I0621 14:43:28.941915       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/grafana-74cd95df6f" duration="83.986¬µs"
I0621 14:43:33.817582       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/grafana-74cd95df6f" duration="41.352241ms"
I0621 14:43:33.819153       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/grafana-74cd95df6f" duration="50.968¬µs"
I0621 14:43:44.620590       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/grafana-74cd95df6f" duration="7.706¬µs"
I0621 14:44:55.460026       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/my-grafana-9c5dd4c77" duration="54.062018ms"
I0621 14:44:55.493730       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/my-grafana-9c5dd4c77" duration="33.625559ms"
I0621 14:44:55.496698       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/my-grafana-9c5dd4c77" duration="151.162¬µs"
I0621 14:46:31.796051       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/sample-microservice-78699ddbd9" duration="101.488¬µs"
I0621 14:46:43.795269       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/sample-microservice-78699ddbd9" duration="160.194¬µs"
I0621 14:48:33.279758       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/my-grafana-9c5dd4c77" duration="10.949¬µs"
I0621 14:49:49.263408       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/grafana-57f4d795bc" duration="186.875651ms"
I0621 14:49:49.302239       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/grafana-57f4d795bc" duration="38.64722ms"
I0621 14:49:49.302339       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/grafana-57f4d795bc" duration="54.925¬µs"
I0621 14:51:34.792553       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/sample-microservice-78699ddbd9" duration="279.566¬µs"
I0621 14:51:47.790743       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/sample-microservice-78699ddbd9" duration="79.963¬µs"
I0621 14:54:56.226509       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/grafana-57f4d795bc" duration="4.875¬µs"
I0621 14:56:36.779275       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/sample-microservice-78699ddbd9" duration="96.35¬µs"
I0621 14:56:47.788994       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/sample-microservice-78699ddbd9" duration="99.385¬µs"


==> kube-controller-manager [7566e71421ad] <==
I0624 10:44:52.584008       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/prometheus-server-5787759b8c" duration="225.266¬µs"
I0624 10:44:57.851396       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/prometheus-prometheus-pushgateway-58cb869bcc" duration="103.529¬µs"
I0624 10:44:59.150315       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/prometheus-kube-state-metrics-67848d7455" duration="92.239¬µs"
I0624 10:44:59.422474       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/prometheus-server-5787759b8c" duration="1.057273ms"
I0624 10:45:02.744341       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/prometheus-kube-state-metrics-67848d7455" duration="32.181595ms"
I0624 10:45:02.750117       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/prometheus-kube-state-metrics-67848d7455" duration="616.102¬µs"
I0624 10:45:12.740896       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/prometheus-prometheus-pushgateway-58cb869bcc" duration="34.043335ms"
I0624 10:45:12.741097       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/prometheus-prometheus-pushgateway-58cb869bcc" duration="94.67¬µs"
I0624 10:45:27.855103       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/prometheus-server-5787759b8c" duration="76.160181ms"
I0624 10:45:27.855284       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/prometheus-server-5787759b8c" duration="98.063¬µs"
I0624 10:45:46.276339       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/prometheus-server-5787759b8c" duration="172.992811ms"
I0624 10:45:46.305962       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/prometheus-server-5787759b8c" duration="29.565462ms"
I0624 10:45:46.306508       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/prometheus-server-5787759b8c" duration="488.753¬µs"
I0624 10:45:47.191911       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/prometheus-server-5787759b8c" duration="86.352¬µs"
I0624 10:45:47.346887       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/prometheus-server-5787759b8c" duration="94.068¬µs"
I0624 10:45:47.616496       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/prometheus-server-5787759b8c" duration="133.098¬µs"
I0624 10:45:47.632499       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/prometheus-server-5787759b8c" duration="112.275¬µs"
I0624 10:45:47.789940       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/prometheus-server-7448577b8c" duration="121.720812ms"
I0624 10:45:47.853183       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/prometheus-server-7448577b8c" duration="62.559381ms"
I0624 10:45:47.854191       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/prometheus-server-7448577b8c" duration="72.459¬µs"
I0624 10:45:47.874975       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/prometheus-server-7448577b8c" duration="187.283¬µs"
I0624 10:46:31.355666       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/sample-microservice-78699ddbd9" duration="56.337¬µs"
I0624 10:46:42.322622       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/sample-microservice-78699ddbd9" duration="23.443¬µs"
I0624 10:49:57.964266       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/prometheus-server-7448577b8c" duration="152.882073ms"
I0624 10:49:58.025461       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/prometheus-server-7448577b8c" duration="60.66467ms"
I0624 10:49:58.030354       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/prometheus-server-7448577b8c" duration="165.681¬µs"
I0624 10:50:00.235519       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/prometheus-server-7448577b8c" duration="383.469¬µs"
I0624 10:50:00.253187       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/prometheus-server-7448577b8c" duration="77.17¬µs"
I0624 10:50:00.256621       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/prometheus-server-7448577b8c" duration="47.794¬µs"
I0624 10:50:01.271562       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/prometheus-server-7448577b8c" duration="152.793¬µs"
I0624 10:50:33.339800       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/prometheus-server-7448577b8c" duration="19.874915ms"
I0624 10:50:33.341092       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/prometheus-server-7448577b8c" duration="136.674¬µs"
I0624 10:51:38.325079       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/sample-microservice-78699ddbd9" duration="80.328¬µs"
I0624 10:51:53.338681       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/sample-microservice-78699ddbd9" duration="39.538¬µs"
I0624 10:53:11.681057       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/my-grafana-754759c9d7" duration="137.368796ms"
I0624 10:53:11.692197       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/my-grafana-754759c9d7" duration="10.977368ms"
I0624 10:53:11.692330       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/my-grafana-754759c9d7" duration="52.662¬µs"
I0624 10:53:11.703555       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/my-grafana-754759c9d7" duration="64.109¬µs"
I0624 10:53:22.837019       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/my-grafana-754759c9d7" duration="159.845¬µs"
I0624 10:53:23.895890       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/my-grafana-754759c9d7" duration="113.247¬µs"
I0624 10:53:32.165571       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/my-grafana-754759c9d7" duration="144.454156ms"
I0624 10:53:32.172579       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="krakend-monitoring/my-grafana-754759c9d7" duration="71.369¬µs"
I0624 10:56:44.330968       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/sample-microservice-78699ddbd9" duration="25.724¬µs"
I0624 10:56:58.371597       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/sample-microservice-78699ddbd9" duration="121.241¬µs"
I0624 11:01:58.354741       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/sample-microservice-78699ddbd9" duration="45.05¬µs"
I0624 11:02:13.344192       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/sample-microservice-78699ddbd9" duration="36.716¬µs"
I0624 11:07:04.336465       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/sample-microservice-78699ddbd9" duration="29.833¬µs"
I0624 11:07:15.338675       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/sample-microservice-78699ddbd9" duration="44.952¬µs"
I0624 11:12:12.326809       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/sample-microservice-78699ddbd9" duration="193.106¬µs"
I0624 11:12:25.341413       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/sample-microservice-78699ddbd9" duration="70.853¬µs"
I0624 11:17:25.343310       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/sample-microservice-78699ddbd9" duration="101.016¬µs"
I0624 11:17:39.337354       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/sample-microservice-78699ddbd9" duration="77.162¬µs"
I0624 11:22:35.338244       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/sample-microservice-78699ddbd9" duration="125.8¬µs"
I0624 11:22:49.339850       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/sample-microservice-78699ddbd9" duration="74.96¬µs"
I0624 11:27:46.318663       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/sample-microservice-78699ddbd9" duration="70.932¬µs"
I0624 11:27:59.336363       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/sample-microservice-78699ddbd9" duration="74.896¬µs"
I0624 11:32:52.343031       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/sample-microservice-78699ddbd9" duration="61.591¬µs"
I0624 11:33:06.337108       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/sample-microservice-78699ddbd9" duration="47.034¬µs"
I0624 11:38:00.324053       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/sample-microservice-78699ddbd9" duration="30.338¬µs"
I0624 11:38:15.342341       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/sample-microservice-78699ddbd9" duration="68.497¬µs"


==> kube-proxy [9d378e1d69e3] <==
I0624 09:33:44.298193       1 server_linux.go:69] "Using iptables proxy"
I0624 09:33:44.443258       1 server.go:1062] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
I0624 09:33:44.719676       1 server.go:659] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0624 09:33:44.719747       1 server_linux.go:165] "Using iptables Proxier"
I0624 09:33:44.728029       1 server_linux.go:511] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0624 09:33:44.728052       1 server_linux.go:528] "Defaulting to no-op detect-local"
I0624 09:33:44.731447       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0624 09:33:44.733127       1 server.go:872] "Version info" version="v1.30.0"
I0624 09:33:44.733144       1 server.go:874] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0624 09:33:44.747729       1 config.go:192] "Starting service config controller"
I0624 09:33:44.748551       1 shared_informer.go:313] Waiting for caches to sync for service config
I0624 09:33:44.748612       1 config.go:101] "Starting endpoint slice config controller"
I0624 09:33:44.748619       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0624 09:33:44.759635       1 config.go:319] "Starting node config controller"
I0624 09:33:44.759707       1 shared_informer.go:313] Waiting for caches to sync for node config
I0624 09:33:44.849205       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0624 09:33:44.860205       1 shared_informer.go:320] Caches are synced for node config
I0624 09:33:44.949088       1 shared_informer.go:320] Caches are synced for service config


==> kube-proxy [f9b77b5e3115] <==
I0621 14:33:19.027197       1 server_linux.go:69] "Using iptables proxy"
I0621 14:33:19.092647       1 server.go:1062] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
I0621 14:33:19.219036       1 server.go:659] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0621 14:33:19.219104       1 server_linux.go:165] "Using iptables Proxier"
I0621 14:33:19.224682       1 server_linux.go:511] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0621 14:33:19.224775       1 server_linux.go:528] "Defaulting to no-op detect-local"
I0621 14:33:19.226772       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0621 14:33:19.228352       1 server.go:872] "Version info" version="v1.30.0"
I0621 14:33:19.228438       1 server.go:874] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0621 14:33:19.241136       1 config.go:192] "Starting service config controller"
I0621 14:33:19.242028       1 shared_informer.go:313] Waiting for caches to sync for service config
I0621 14:33:19.242124       1 config.go:101] "Starting endpoint slice config controller"
I0621 14:33:19.242135       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0621 14:33:19.246290       1 config.go:319] "Starting node config controller"
I0621 14:33:19.246315       1 shared_informer.go:313] Waiting for caches to sync for node config
I0621 14:33:19.343079       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0621 14:33:19.343141       1 shared_informer.go:320] Caches are synced for service config
I0621 14:33:19.346655       1 shared_informer.go:320] Caches are synced for node config


==> kube-scheduler [94565ea85813] <==
I0624 09:33:33.759925       1 serving.go:380] Generated self-signed cert in-memory
W0624 09:33:38.781835       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0624 09:33:38.781888       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0624 09:33:38.781900       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0624 09:33:38.781925       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0624 09:33:38.886847       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.30.0"
I0624 09:33:38.886882       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0624 09:33:38.907765       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0624 09:33:38.907896       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0624 09:33:38.907909       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0624 09:33:38.909130       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0624 09:33:39.011237       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kube-scheduler [f7d0a6ab14d5] <==
I0621 14:33:09.582751       1 serving.go:380] Generated self-signed cert in-memory
W0621 14:33:13.097918       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0621 14:33:13.097955       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0621 14:33:13.097968       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0621 14:33:13.097976       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0621 14:33:13.248256       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.30.0"
I0621 14:33:13.248274       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0621 14:33:13.257275       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0621 14:33:13.264694       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0621 14:33:13.264717       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0621 14:33:13.268896       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0621 14:33:13.473317       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0621 15:01:01.004733       1 secure_serving.go:258] Stopped listening on 127.0.0.1:10259
I0621 15:01:01.050604       1 configmap_cafile_content.go:223] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0621 15:01:01.000179       1 tlsconfig.go:255] "Shutting down DynamicServingCertificateController"
E0621 15:01:01.069257       1 run.go:74] "command failed" err="finished without leader elect"


==> kubelet <==
Jun 24 11:37:24 minikube kubelet[1211]: E0624 11:37:24.305591    1211 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"vaultwarden\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=vaultwarden pod=vaultwarden-release-0_vaulwarden(45634c83-1229-4e3b-bf40-7ce7d708c9bd)\"" pod="vaulwarden/vaultwarden-release-0" podUID="45634c83-1229-4e3b-bf40-7ce7d708c9bd"
Jun 24 11:37:33 minikube kubelet[1211]: E0624 11:37:33.306896    1211 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"sample-microservice\" with ImagePullBackOff: \"Back-off pulling image \\\"sample-microservice:latest\\\"\"" pod="default/sample-microservice-78699ddbd9-xgbxl" podUID="aa4d6184-7749-46f0-9c87-0ebdc2d5ff6d"
Jun 24 11:37:38 minikube kubelet[1211]: I0624 11:37:38.303548    1211 scope.go:117] "RemoveContainer" containerID="f2c39c9137f54a10aa0d721cb9f2cbe746c87a1800a677eba497e180d078cde8"
Jun 24 11:37:38 minikube kubelet[1211]: E0624 11:37:38.305988    1211 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"vaultwarden\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=vaultwarden pod=vaultwarden-release-0_vaulwarden(45634c83-1229-4e3b-bf40-7ce7d708c9bd)\"" pod="vaulwarden/vaultwarden-release-0" podUID="45634c83-1229-4e3b-bf40-7ce7d708c9bd"
Jun 24 11:37:47 minikube kubelet[1211]: E0624 11:37:47.497090    1211 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for sample-microservice, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="sample-microservice:latest"
Jun 24 11:37:47 minikube kubelet[1211]: E0624 11:37:47.498249    1211 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for sample-microservice, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="sample-microservice:latest"
Jun 24 11:37:47 minikube kubelet[1211]: E0624 11:37:47.498444    1211 kuberuntime_manager.go:1256] container &Container{Name:sample-microservice,Image:sample-microservice:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-458hq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod sample-microservice-78699ddbd9-xgbxl_default(aa4d6184-7749-46f0-9c87-0ebdc2d5ff6d): ErrImagePull: Error response from daemon: pull access denied for sample-microservice, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Jun 24 11:37:47 minikube kubelet[1211]: E0624 11:37:47.498687    1211 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"sample-microservice\" with ErrImagePull: \"Error response from daemon: pull access denied for sample-microservice, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/sample-microservice-78699ddbd9-xgbxl" podUID="aa4d6184-7749-46f0-9c87-0ebdc2d5ff6d"
Jun 24 11:37:51 minikube kubelet[1211]: I0624 11:37:51.304206    1211 scope.go:117] "RemoveContainer" containerID="f2c39c9137f54a10aa0d721cb9f2cbe746c87a1800a677eba497e180d078cde8"
Jun 24 11:37:51 minikube kubelet[1211]: E0624 11:37:51.304688    1211 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"vaultwarden\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=vaultwarden pod=vaultwarden-release-0_vaulwarden(45634c83-1229-4e3b-bf40-7ce7d708c9bd)\"" pod="vaulwarden/vaultwarden-release-0" podUID="45634c83-1229-4e3b-bf40-7ce7d708c9bd"
Jun 24 11:38:00 minikube kubelet[1211]: E0624 11:38:00.303771    1211 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"sample-microservice\" with ImagePullBackOff: \"Back-off pulling image \\\"sample-microservice:latest\\\"\"" pod="default/sample-microservice-78699ddbd9-xgbxl" podUID="aa4d6184-7749-46f0-9c87-0ebdc2d5ff6d"
Jun 24 11:38:02 minikube kubelet[1211]: I0624 11:38:02.303913    1211 scope.go:117] "RemoveContainer" containerID="f2c39c9137f54a10aa0d721cb9f2cbe746c87a1800a677eba497e180d078cde8"
Jun 24 11:38:02 minikube kubelet[1211]: E0624 11:38:02.305004    1211 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"vaultwarden\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=vaultwarden pod=vaultwarden-release-0_vaulwarden(45634c83-1229-4e3b-bf40-7ce7d708c9bd)\"" pod="vaulwarden/vaultwarden-release-0" podUID="45634c83-1229-4e3b-bf40-7ce7d708c9bd"
Jun 24 11:38:14 minikube kubelet[1211]: I0624 11:38:14.303468    1211 scope.go:117] "RemoveContainer" containerID="f2c39c9137f54a10aa0d721cb9f2cbe746c87a1800a677eba497e180d078cde8"
Jun 24 11:38:14 minikube kubelet[1211]: E0624 11:38:14.304654    1211 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"vaultwarden\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=vaultwarden pod=vaultwarden-release-0_vaulwarden(45634c83-1229-4e3b-bf40-7ce7d708c9bd)\"" pod="vaulwarden/vaultwarden-release-0" podUID="45634c83-1229-4e3b-bf40-7ce7d708c9bd"
Jun 24 11:38:15 minikube kubelet[1211]: E0624 11:38:15.314727    1211 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"sample-microservice\" with ImagePullBackOff: \"Back-off pulling image \\\"sample-microservice:latest\\\"\"" pod="default/sample-microservice-78699ddbd9-xgbxl" podUID="aa4d6184-7749-46f0-9c87-0ebdc2d5ff6d"
Jun 24 11:38:25 minikube kubelet[1211]: I0624 11:38:25.302583    1211 scope.go:117] "RemoveContainer" containerID="f2c39c9137f54a10aa0d721cb9f2cbe746c87a1800a677eba497e180d078cde8"
Jun 24 11:38:25 minikube kubelet[1211]: E0624 11:38:25.303025    1211 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"vaultwarden\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=vaultwarden pod=vaultwarden-release-0_vaulwarden(45634c83-1229-4e3b-bf40-7ce7d708c9bd)\"" pod="vaulwarden/vaultwarden-release-0" podUID="45634c83-1229-4e3b-bf40-7ce7d708c9bd"
Jun 24 11:38:29 minikube kubelet[1211]: E0624 11:38:29.335147    1211 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"sample-microservice\" with ImagePullBackOff: \"Back-off pulling image \\\"sample-microservice:latest\\\"\"" pod="default/sample-microservice-78699ddbd9-xgbxl" podUID="aa4d6184-7749-46f0-9c87-0ebdc2d5ff6d"
Jun 24 11:38:38 minikube kubelet[1211]: I0624 11:38:38.302136    1211 scope.go:117] "RemoveContainer" containerID="f2c39c9137f54a10aa0d721cb9f2cbe746c87a1800a677eba497e180d078cde8"
Jun 24 11:38:38 minikube kubelet[1211]: E0624 11:38:38.302801    1211 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"vaultwarden\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=vaultwarden pod=vaultwarden-release-0_vaulwarden(45634c83-1229-4e3b-bf40-7ce7d708c9bd)\"" pod="vaulwarden/vaultwarden-release-0" podUID="45634c83-1229-4e3b-bf40-7ce7d708c9bd"
Jun 24 11:38:41 minikube kubelet[1211]: E0624 11:38:41.322544    1211 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"sample-microservice\" with ImagePullBackOff: \"Back-off pulling image \\\"sample-microservice:latest\\\"\"" pod="default/sample-microservice-78699ddbd9-xgbxl" podUID="aa4d6184-7749-46f0-9c87-0ebdc2d5ff6d"
Jun 24 11:38:52 minikube kubelet[1211]: E0624 11:38:52.308306    1211 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"sample-microservice\" with ImagePullBackOff: \"Back-off pulling image \\\"sample-microservice:latest\\\"\"" pod="default/sample-microservice-78699ddbd9-xgbxl" podUID="aa4d6184-7749-46f0-9c87-0ebdc2d5ff6d"
Jun 24 11:38:53 minikube kubelet[1211]: I0624 11:38:53.307248    1211 scope.go:117] "RemoveContainer" containerID="f2c39c9137f54a10aa0d721cb9f2cbe746c87a1800a677eba497e180d078cde8"
Jun 24 11:38:53 minikube kubelet[1211]: E0624 11:38:53.308016    1211 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"vaultwarden\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=vaultwarden pod=vaultwarden-release-0_vaulwarden(45634c83-1229-4e3b-bf40-7ce7d708c9bd)\"" pod="vaulwarden/vaultwarden-release-0" podUID="45634c83-1229-4e3b-bf40-7ce7d708c9bd"
Jun 24 11:39:05 minikube kubelet[1211]: E0624 11:39:05.308790    1211 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"sample-microservice\" with ImagePullBackOff: \"Back-off pulling image \\\"sample-microservice:latest\\\"\"" pod="default/sample-microservice-78699ddbd9-xgbxl" podUID="aa4d6184-7749-46f0-9c87-0ebdc2d5ff6d"
Jun 24 11:39:07 minikube kubelet[1211]: I0624 11:39:07.304211    1211 scope.go:117] "RemoveContainer" containerID="f2c39c9137f54a10aa0d721cb9f2cbe746c87a1800a677eba497e180d078cde8"
Jun 24 11:39:07 minikube kubelet[1211]: E0624 11:39:07.306222    1211 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"vaultwarden\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=vaultwarden pod=vaultwarden-release-0_vaulwarden(45634c83-1229-4e3b-bf40-7ce7d708c9bd)\"" pod="vaulwarden/vaultwarden-release-0" podUID="45634c83-1229-4e3b-bf40-7ce7d708c9bd"
Jun 24 11:39:16 minikube kubelet[1211]: E0624 11:39:16.308280    1211 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"sample-microservice\" with ImagePullBackOff: \"Back-off pulling image \\\"sample-microservice:latest\\\"\"" pod="default/sample-microservice-78699ddbd9-xgbxl" podUID="aa4d6184-7749-46f0-9c87-0ebdc2d5ff6d"
Jun 24 11:39:18 minikube kubelet[1211]: I0624 11:39:18.310873    1211 scope.go:117] "RemoveContainer" containerID="f2c39c9137f54a10aa0d721cb9f2cbe746c87a1800a677eba497e180d078cde8"
Jun 24 11:39:18 minikube kubelet[1211]: E0624 11:39:18.312836    1211 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"vaultwarden\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=vaultwarden pod=vaultwarden-release-0_vaulwarden(45634c83-1229-4e3b-bf40-7ce7d708c9bd)\"" pod="vaulwarden/vaultwarden-release-0" podUID="45634c83-1229-4e3b-bf40-7ce7d708c9bd"
Jun 24 11:39:28 minikube kubelet[1211]: E0624 11:39:28.305642    1211 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"sample-microservice\" with ImagePullBackOff: \"Back-off pulling image \\\"sample-microservice:latest\\\"\"" pod="default/sample-microservice-78699ddbd9-xgbxl" podUID="aa4d6184-7749-46f0-9c87-0ebdc2d5ff6d"
Jun 24 11:39:29 minikube kubelet[1211]: I0624 11:39:29.306289    1211 scope.go:117] "RemoveContainer" containerID="f2c39c9137f54a10aa0d721cb9f2cbe746c87a1800a677eba497e180d078cde8"
Jun 24 11:39:29 minikube kubelet[1211]: E0624 11:39:29.307681    1211 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"vaultwarden\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=vaultwarden pod=vaultwarden-release-0_vaulwarden(45634c83-1229-4e3b-bf40-7ce7d708c9bd)\"" pod="vaulwarden/vaultwarden-release-0" podUID="45634c83-1229-4e3b-bf40-7ce7d708c9bd"
Jun 24 11:39:40 minikube kubelet[1211]: E0624 11:39:40.306892    1211 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"sample-microservice\" with ImagePullBackOff: \"Back-off pulling image \\\"sample-microservice:latest\\\"\"" pod="default/sample-microservice-78699ddbd9-xgbxl" podUID="aa4d6184-7749-46f0-9c87-0ebdc2d5ff6d"
Jun 24 11:39:42 minikube kubelet[1211]: I0624 11:39:42.302943    1211 scope.go:117] "RemoveContainer" containerID="f2c39c9137f54a10aa0d721cb9f2cbe746c87a1800a677eba497e180d078cde8"
Jun 24 11:39:42 minikube kubelet[1211]: E0624 11:39:42.303394    1211 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"vaultwarden\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=vaultwarden pod=vaultwarden-release-0_vaulwarden(45634c83-1229-4e3b-bf40-7ce7d708c9bd)\"" pod="vaulwarden/vaultwarden-release-0" podUID="45634c83-1229-4e3b-bf40-7ce7d708c9bd"
Jun 24 11:39:52 minikube kubelet[1211]: E0624 11:39:52.304933    1211 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"sample-microservice\" with ImagePullBackOff: \"Back-off pulling image \\\"sample-microservice:latest\\\"\"" pod="default/sample-microservice-78699ddbd9-xgbxl" podUID="aa4d6184-7749-46f0-9c87-0ebdc2d5ff6d"
Jun 24 11:39:56 minikube kubelet[1211]: I0624 11:39:56.302576    1211 scope.go:117] "RemoveContainer" containerID="f2c39c9137f54a10aa0d721cb9f2cbe746c87a1800a677eba497e180d078cde8"
Jun 24 11:39:56 minikube kubelet[1211]: E0624 11:39:56.303067    1211 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"vaultwarden\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=vaultwarden pod=vaultwarden-release-0_vaulwarden(45634c83-1229-4e3b-bf40-7ce7d708c9bd)\"" pod="vaulwarden/vaultwarden-release-0" podUID="45634c83-1229-4e3b-bf40-7ce7d708c9bd"
Jun 24 11:40:07 minikube kubelet[1211]: E0624 11:40:07.306141    1211 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"sample-microservice\" with ImagePullBackOff: \"Back-off pulling image \\\"sample-microservice:latest\\\"\"" pod="default/sample-microservice-78699ddbd9-xgbxl" podUID="aa4d6184-7749-46f0-9c87-0ebdc2d5ff6d"
Jun 24 11:40:09 minikube kubelet[1211]: I0624 11:40:09.303017    1211 scope.go:117] "RemoveContainer" containerID="f2c39c9137f54a10aa0d721cb9f2cbe746c87a1800a677eba497e180d078cde8"
Jun 24 11:40:09 minikube kubelet[1211]: E0624 11:40:09.304526    1211 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"vaultwarden\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=vaultwarden pod=vaultwarden-release-0_vaulwarden(45634c83-1229-4e3b-bf40-7ce7d708c9bd)\"" pod="vaulwarden/vaultwarden-release-0" podUID="45634c83-1229-4e3b-bf40-7ce7d708c9bd"
Jun 24 11:40:22 minikube kubelet[1211]: E0624 11:40:22.304000    1211 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"sample-microservice\" with ImagePullBackOff: \"Back-off pulling image \\\"sample-microservice:latest\\\"\"" pod="default/sample-microservice-78699ddbd9-xgbxl" podUID="aa4d6184-7749-46f0-9c87-0ebdc2d5ff6d"
Jun 24 11:40:23 minikube kubelet[1211]: I0624 11:40:23.305053    1211 scope.go:117] "RemoveContainer" containerID="f2c39c9137f54a10aa0d721cb9f2cbe746c87a1800a677eba497e180d078cde8"
Jun 24 11:40:23 minikube kubelet[1211]: E0624 11:40:23.306457    1211 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"vaultwarden\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=vaultwarden pod=vaultwarden-release-0_vaulwarden(45634c83-1229-4e3b-bf40-7ce7d708c9bd)\"" pod="vaulwarden/vaultwarden-release-0" podUID="45634c83-1229-4e3b-bf40-7ce7d708c9bd"
Jun 24 11:40:35 minikube kubelet[1211]: I0624 11:40:35.303035    1211 scope.go:117] "RemoveContainer" containerID="f2c39c9137f54a10aa0d721cb9f2cbe746c87a1800a677eba497e180d078cde8"
Jun 24 11:40:35 minikube kubelet[1211]: E0624 11:40:35.304192    1211 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"vaultwarden\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=vaultwarden pod=vaultwarden-release-0_vaulwarden(45634c83-1229-4e3b-bf40-7ce7d708c9bd)\"" pod="vaulwarden/vaultwarden-release-0" podUID="45634c83-1229-4e3b-bf40-7ce7d708c9bd"
Jun 24 11:40:37 minikube kubelet[1211]: E0624 11:40:37.309229    1211 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"sample-microservice\" with ImagePullBackOff: \"Back-off pulling image \\\"sample-microservice:latest\\\"\"" pod="default/sample-microservice-78699ddbd9-xgbxl" podUID="aa4d6184-7749-46f0-9c87-0ebdc2d5ff6d"
Jun 24 11:40:46 minikube kubelet[1211]: I0624 11:40:46.303628    1211 scope.go:117] "RemoveContainer" containerID="f2c39c9137f54a10aa0d721cb9f2cbe746c87a1800a677eba497e180d078cde8"
Jun 24 11:40:46 minikube kubelet[1211]: E0624 11:40:46.304133    1211 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"vaultwarden\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=vaultwarden pod=vaultwarden-release-0_vaulwarden(45634c83-1229-4e3b-bf40-7ce7d708c9bd)\"" pod="vaulwarden/vaultwarden-release-0" podUID="45634c83-1229-4e3b-bf40-7ce7d708c9bd"
Jun 24 11:40:49 minikube kubelet[1211]: E0624 11:40:49.308493    1211 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"sample-microservice\" with ImagePullBackOff: \"Back-off pulling image \\\"sample-microservice:latest\\\"\"" pod="default/sample-microservice-78699ddbd9-xgbxl" podUID="aa4d6184-7749-46f0-9c87-0ebdc2d5ff6d"
Jun 24 11:40:57 minikube kubelet[1211]: I0624 11:40:57.304294    1211 scope.go:117] "RemoveContainer" containerID="f2c39c9137f54a10aa0d721cb9f2cbe746c87a1800a677eba497e180d078cde8"
Jun 24 11:40:57 minikube kubelet[1211]: E0624 11:40:57.304783    1211 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"vaultwarden\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=vaultwarden pod=vaultwarden-release-0_vaulwarden(45634c83-1229-4e3b-bf40-7ce7d708c9bd)\"" pod="vaulwarden/vaultwarden-release-0" podUID="45634c83-1229-4e3b-bf40-7ce7d708c9bd"
Jun 24 11:41:02 minikube kubelet[1211]: E0624 11:41:02.306752    1211 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"sample-microservice\" with ImagePullBackOff: \"Back-off pulling image \\\"sample-microservice:latest\\\"\"" pod="default/sample-microservice-78699ddbd9-xgbxl" podUID="aa4d6184-7749-46f0-9c87-0ebdc2d5ff6d"
Jun 24 11:41:12 minikube kubelet[1211]: I0624 11:41:12.302989    1211 scope.go:117] "RemoveContainer" containerID="f2c39c9137f54a10aa0d721cb9f2cbe746c87a1800a677eba497e180d078cde8"
Jun 24 11:41:12 minikube kubelet[1211]: E0624 11:41:12.303648    1211 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"vaultwarden\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=vaultwarden pod=vaultwarden-release-0_vaulwarden(45634c83-1229-4e3b-bf40-7ce7d708c9bd)\"" pod="vaulwarden/vaultwarden-release-0" podUID="45634c83-1229-4e3b-bf40-7ce7d708c9bd"
Jun 24 11:41:15 minikube kubelet[1211]: E0624 11:41:15.306808    1211 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"sample-microservice\" with ImagePullBackOff: \"Back-off pulling image \\\"sample-microservice:latest\\\"\"" pod="default/sample-microservice-78699ddbd9-xgbxl" podUID="aa4d6184-7749-46f0-9c87-0ebdc2d5ff6d"
Jun 24 11:41:24 minikube kubelet[1211]: I0624 11:41:24.302440    1211 scope.go:117] "RemoveContainer" containerID="f2c39c9137f54a10aa0d721cb9f2cbe746c87a1800a677eba497e180d078cde8"
Jun 24 11:41:24 minikube kubelet[1211]: E0624 11:41:24.302852    1211 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"vaultwarden\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=vaultwarden pod=vaultwarden-release-0_vaulwarden(45634c83-1229-4e3b-bf40-7ce7d708c9bd)\"" pod="vaulwarden/vaultwarden-release-0" podUID="45634c83-1229-4e3b-bf40-7ce7d708c9bd"


==> kubernetes-dashboard [6ef0ad7646f0] <==
2024/06/21 14:33:27 Using namespace: kubernetes-dashboard
2024/06/21 14:33:27 Using in-cluster config to connect to apiserver
2024/06/21 14:33:28 Using secret token for csrf signing
2024/06/21 14:33:28 Initializing csrf token from kubernetes-dashboard-csrf secret
2024/06/21 14:33:28 Empty token. Generating and storing in a secret kubernetes-dashboard-csrf
2024/06/21 14:33:28 Successful initial request to the apiserver, version: v1.30.0
2024/06/21 14:33:28 Generating JWE encryption key
2024/06/21 14:33:28 New synchronizer has been registered: kubernetes-dashboard-key-holder-kubernetes-dashboard. Starting
2024/06/21 14:33:28 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2024/06/21 14:33:28 Initializing JWE encryption key from synchronized object
2024/06/21 14:33:28 Creating in-cluster Sidecar client
2024/06/21 14:33:28 Serving insecurely on HTTP port: 9090
2024/06/21 14:33:28 Successful request to sidecar
2024/06/21 14:33:27 Starting overwatch


==> kubernetes-dashboard [fe95a9ff690d] <==
2024/06/24 09:33:49 Using namespace: kubernetes-dashboard
2024/06/24 09:33:49 Using in-cluster config to connect to apiserver
2024/06/24 09:33:49 Using secret token for csrf signing
2024/06/24 09:33:49 Initializing csrf token from kubernetes-dashboard-csrf secret
2024/06/24 09:33:50 Empty token. Generating and storing in a secret kubernetes-dashboard-csrf
2024/06/24 09:33:50 Successful initial request to the apiserver, version: v1.30.0
2024/06/24 09:33:50 Generating JWE encryption key
2024/06/24 09:33:50 New synchronizer has been registered: kubernetes-dashboard-key-holder-kubernetes-dashboard. Starting
2024/06/24 09:33:50 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2024/06/24 09:33:50 Initializing JWE encryption key from synchronized object
2024/06/24 09:33:50 Creating in-cluster Sidecar client
2024/06/24 09:33:50 Metric client health check failed: the server is currently unable to handle the request (get services dashboard-metrics-scraper). Retrying in 30 seconds.
2024/06/24 09:33:50 Serving insecurely on HTTP port: 9090
2024/06/24 09:34:20 Successful request to sidecar
2024/06/24 09:33:49 Starting overwatch


==> storage-provisioner [6764bcc8bb8c] <==
I0624 09:34:37.929564       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0624 09:34:37.931116       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"94954cd4-18ae-4b41-9f26-5e295b7ddef6", APIVersion:"v1", ResourceVersion:"37636", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_775709a2-d0a1-40de-9aa9-d27ca1e50b36 became leader
I0624 09:34:37.931281       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_775709a2-d0a1-40de-9aa9-d27ca1e50b36!
I0624 09:34:38.035414       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_775709a2-d0a1-40de-9aa9-d27ca1e50b36!
I0624 09:34:38.036108       1 controller.go:1472] delete "pvc-aa09cfb0-3684-4993-857b-009b28e89740": started
I0624 09:34:38.036157       1 storage_provisioner.go:98] Deleting volume &PersistentVolume{ObjectMeta:{pvc-aa09cfb0-3684-4993-857b-009b28e89740    bfbd3236-def3-4a4c-8306-baf7242332fc 30187 0 2024-06-19 14:19:42 +0000 UTC <nil> <nil> map[] map[hostPathProvisionerIdentity:c540c508-262f-46d9-8ee0-a2360bf54623 pv.kubernetes.io/provisioned-by:k8s.io/minikube-hostpath] [] [kubernetes.io/pv-protection]  [{storage-provisioner Update v1 2024-06-19 14:19:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:hostPathProvisionerIdentity":{},"f:pv.kubernetes.io/provisioned-by":{}}},"f:spec":{"f:accessModes":{},"f:capacity":{".":{},"f:storage":{}},"f:claimRef":{".":{},"f:apiVersion":{},"f:kind":{},"f:name":{},"f:namespace":{},"f:resourceVersion":{},"f:uid":{}},"f:hostPath":{".":{},"f:path":{},"f:type":{}},"f:persistentVolumeReclaimPolicy":{},"f:storageClassName":{},"f:volumeMode":{}}}} {kube-controller-manager Update v1 2024-06-21 10:21:43 +0000 UTC FieldsV1 {"f:status":{"f:phase":{}}}}]},Spec:PersistentVolumeSpec{Capacity:ResourceList{storage: {{8589934592 0} {<nil>}  BinarySI},},PersistentVolumeSource:PersistentVolumeSource{GCEPersistentDisk:nil,AWSElasticBlockStore:nil,HostPath:&HostPathVolumeSource{Path:/tmp/hostpath-provisioner/default/data-my-postgresql-0,Type:*,},Glusterfs:nil,NFS:nil,RBD:nil,ISCSI:nil,Cinder:nil,CephFS:nil,FC:nil,Flocker:nil,FlexVolume:nil,AzureFile:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Local:nil,StorageOS:nil,CSI:nil,},AccessModes:[ReadWriteOnce],ClaimRef:&ObjectReference{Kind:PersistentVolumeClaim,Namespace:default,Name:data-my-postgresql-0,UID:aa09cfb0-3684-4993-857b-009b28e89740,APIVersion:v1,ResourceVersion:7073,FieldPath:,},PersistentVolumeReclaimPolicy:Delete,StorageClassName:standard,MountOptions:[],VolumeMode:*Filesystem,NodeAffinity:nil,},Status:PersistentVolumeStatus{Phase:Released,Message:,Reason:,},}
I0624 09:34:38.045957       1 controller.go:1478] delete "pvc-aa09cfb0-3684-4993-857b-009b28e89740": volume deletion ignored: ignored because identity annotation on PV does not match ours
I0624 09:49:37.972737       1 controller.go:1472] delete "pvc-aa09cfb0-3684-4993-857b-009b28e89740": started
I0624 09:49:37.972961       1 storage_provisioner.go:98] Deleting volume &PersistentVolume{ObjectMeta:{pvc-aa09cfb0-3684-4993-857b-009b28e89740    bfbd3236-def3-4a4c-8306-baf7242332fc 30187 0 2024-06-19 14:19:42 +0000 UTC <nil> <nil> map[] map[hostPathProvisionerIdentity:c540c508-262f-46d9-8ee0-a2360bf54623 pv.kubernetes.io/provisioned-by:k8s.io/minikube-hostpath] [] [kubernetes.io/pv-protection]  [{storage-provisioner Update v1 2024-06-19 14:19:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:hostPathProvisionerIdentity":{},"f:pv.kubernetes.io/provisioned-by":{}}},"f:spec":{"f:accessModes":{},"f:capacity":{".":{},"f:storage":{}},"f:claimRef":{".":{},"f:apiVersion":{},"f:kind":{},"f:name":{},"f:namespace":{},"f:resourceVersion":{},"f:uid":{}},"f:hostPath":{".":{},"f:path":{},"f:type":{}},"f:persistentVolumeReclaimPolicy":{},"f:storageClassName":{},"f:volumeMode":{}}}} {kube-controller-manager Update v1 2024-06-21 10:21:43 +0000 UTC FieldsV1 {"f:status":{"f:phase":{}}}}]},Spec:PersistentVolumeSpec{Capacity:ResourceList{storage: {{8589934592 0} {<nil>}  BinarySI},},PersistentVolumeSource:PersistentVolumeSource{GCEPersistentDisk:nil,AWSElasticBlockStore:nil,HostPath:&HostPathVolumeSource{Path:/tmp/hostpath-provisioner/default/data-my-postgresql-0,Type:*,},Glusterfs:nil,NFS:nil,RBD:nil,ISCSI:nil,Cinder:nil,CephFS:nil,FC:nil,Flocker:nil,FlexVolume:nil,AzureFile:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Local:nil,StorageOS:nil,CSI:nil,},AccessModes:[ReadWriteOnce],ClaimRef:&ObjectReference{Kind:PersistentVolumeClaim,Namespace:default,Name:data-my-postgresql-0,UID:aa09cfb0-3684-4993-857b-009b28e89740,APIVersion:v1,ResourceVersion:7073,FieldPath:,},PersistentVolumeReclaimPolicy:Delete,StorageClassName:standard,MountOptions:[],VolumeMode:*Filesystem,NodeAffinity:nil,},Status:PersistentVolumeStatus{Phase:Released,Message:,Reason:,},}
I0624 09:49:37.978750       1 controller.go:1478] delete "pvc-aa09cfb0-3684-4993-857b-009b28e89740": volume deletion ignored: ignored because identity annotation on PV does not match ours
I0624 10:04:37.970317       1 controller.go:1472] delete "pvc-aa09cfb0-3684-4993-857b-009b28e89740": started
I0624 10:04:37.970443       1 storage_provisioner.go:98] Deleting volume &PersistentVolume{ObjectMeta:{pvc-aa09cfb0-3684-4993-857b-009b28e89740    bfbd3236-def3-4a4c-8306-baf7242332fc 30187 0 2024-06-19 14:19:42 +0000 UTC <nil> <nil> map[] map[hostPathProvisionerIdentity:c540c508-262f-46d9-8ee0-a2360bf54623 pv.kubernetes.io/provisioned-by:k8s.io/minikube-hostpath] [] [kubernetes.io/pv-protection]  [{storage-provisioner Update v1 2024-06-19 14:19:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:hostPathProvisionerIdentity":{},"f:pv.kubernetes.io/provisioned-by":{}}},"f:spec":{"f:accessModes":{},"f:capacity":{".":{},"f:storage":{}},"f:claimRef":{".":{},"f:apiVersion":{},"f:kind":{},"f:name":{},"f:namespace":{},"f:resourceVersion":{},"f:uid":{}},"f:hostPath":{".":{},"f:path":{},"f:type":{}},"f:persistentVolumeReclaimPolicy":{},"f:storageClassName":{},"f:volumeMode":{}}}} {kube-controller-manager Update v1 2024-06-21 10:21:43 +0000 UTC FieldsV1 {"f:status":{"f:phase":{}}}}]},Spec:PersistentVolumeSpec{Capacity:ResourceList{storage: {{8589934592 0} {<nil>}  BinarySI},},PersistentVolumeSource:PersistentVolumeSource{GCEPersistentDisk:nil,AWSElasticBlockStore:nil,HostPath:&HostPathVolumeSource{Path:/tmp/hostpath-provisioner/default/data-my-postgresql-0,Type:*,},Glusterfs:nil,NFS:nil,RBD:nil,ISCSI:nil,Cinder:nil,CephFS:nil,FC:nil,Flocker:nil,FlexVolume:nil,AzureFile:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Local:nil,StorageOS:nil,CSI:nil,},AccessModes:[ReadWriteOnce],ClaimRef:&ObjectReference{Kind:PersistentVolumeClaim,Namespace:default,Name:data-my-postgresql-0,UID:aa09cfb0-3684-4993-857b-009b28e89740,APIVersion:v1,ResourceVersion:7073,FieldPath:,},PersistentVolumeReclaimPolicy:Delete,StorageClassName:standard,MountOptions:[],VolumeMode:*Filesystem,NodeAffinity:nil,},Status:PersistentVolumeStatus{Phase:Released,Message:,Reason:,},}
I0624 10:04:37.976696       1 controller.go:1478] delete "pvc-aa09cfb0-3684-4993-857b-009b28e89740": volume deletion ignored: ignored because identity annotation on PV does not match ours
I0624 10:19:37.970703       1 controller.go:1472] delete "pvc-aa09cfb0-3684-4993-857b-009b28e89740": started
I0624 10:19:37.971554       1 storage_provisioner.go:98] Deleting volume &PersistentVolume{ObjectMeta:{pvc-aa09cfb0-3684-4993-857b-009b28e89740    bfbd3236-def3-4a4c-8306-baf7242332fc 30187 0 2024-06-19 14:19:42 +0000 UTC <nil> <nil> map[] map[hostPathProvisionerIdentity:c540c508-262f-46d9-8ee0-a2360bf54623 pv.kubernetes.io/provisioned-by:k8s.io/minikube-hostpath] [] [kubernetes.io/pv-protection]  [{storage-provisioner Update v1 2024-06-19 14:19:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:hostPathProvisionerIdentity":{},"f:pv.kubernetes.io/provisioned-by":{}}},"f:spec":{"f:accessModes":{},"f:capacity":{".":{},"f:storage":{}},"f:claimRef":{".":{},"f:apiVersion":{},"f:kind":{},"f:name":{},"f:namespace":{},"f:resourceVersion":{},"f:uid":{}},"f:hostPath":{".":{},"f:path":{},"f:type":{}},"f:persistentVolumeReclaimPolicy":{},"f:storageClassName":{},"f:volumeMode":{}}}} {kube-controller-manager Update v1 2024-06-21 10:21:43 +0000 UTC FieldsV1 {"f:status":{"f:phase":{}}}}]},Spec:PersistentVolumeSpec{Capacity:ResourceList{storage: {{8589934592 0} {<nil>}  BinarySI},},PersistentVolumeSource:PersistentVolumeSource{GCEPersistentDisk:nil,AWSElasticBlockStore:nil,HostPath:&HostPathVolumeSource{Path:/tmp/hostpath-provisioner/default/data-my-postgresql-0,Type:*,},Glusterfs:nil,NFS:nil,RBD:nil,ISCSI:nil,Cinder:nil,CephFS:nil,FC:nil,Flocker:nil,FlexVolume:nil,AzureFile:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Local:nil,StorageOS:nil,CSI:nil,},AccessModes:[ReadWriteOnce],ClaimRef:&ObjectReference{Kind:PersistentVolumeClaim,Namespace:default,Name:data-my-postgresql-0,UID:aa09cfb0-3684-4993-857b-009b28e89740,APIVersion:v1,ResourceVersion:7073,FieldPath:,},PersistentVolumeReclaimPolicy:Delete,StorageClassName:standard,MountOptions:[],VolumeMode:*Filesystem,NodeAffinity:nil,},Status:PersistentVolumeStatus{Phase:Released,Message:,Reason:,},}
I0624 10:19:37.976917       1 controller.go:1478] delete "pvc-aa09cfb0-3684-4993-857b-009b28e89740": volume deletion ignored: ignored because identity annotation on PV does not match ours
I0624 10:34:37.970074       1 controller.go:1472] delete "pvc-aa09cfb0-3684-4993-857b-009b28e89740": started
I0624 10:34:37.970173       1 storage_provisioner.go:98] Deleting volume &PersistentVolume{ObjectMeta:{pvc-aa09cfb0-3684-4993-857b-009b28e89740    bfbd3236-def3-4a4c-8306-baf7242332fc 30187 0 2024-06-19 14:19:42 +0000 UTC <nil> <nil> map[] map[hostPathProvisionerIdentity:c540c508-262f-46d9-8ee0-a2360bf54623 pv.kubernetes.io/provisioned-by:k8s.io/minikube-hostpath] [] [kubernetes.io/pv-protection]  [{storage-provisioner Update v1 2024-06-19 14:19:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:hostPathProvisionerIdentity":{},"f:pv.kubernetes.io/provisioned-by":{}}},"f:spec":{"f:accessModes":{},"f:capacity":{".":{},"f:storage":{}},"f:claimRef":{".":{},"f:apiVersion":{},"f:kind":{},"f:name":{},"f:namespace":{},"f:resourceVersion":{},"f:uid":{}},"f:hostPath":{".":{},"f:path":{},"f:type":{}},"f:persistentVolumeReclaimPolicy":{},"f:storageClassName":{},"f:volumeMode":{}}}} {kube-controller-manager Update v1 2024-06-21 10:21:43 +0000 UTC FieldsV1 {"f:status":{"f:phase":{}}}}]},Spec:PersistentVolumeSpec{Capacity:ResourceList{storage: {{8589934592 0} {<nil>}  BinarySI},},PersistentVolumeSource:PersistentVolumeSource{GCEPersistentDisk:nil,AWSElasticBlockStore:nil,HostPath:&HostPathVolumeSource{Path:/tmp/hostpath-provisioner/default/data-my-postgresql-0,Type:*,},Glusterfs:nil,NFS:nil,RBD:nil,ISCSI:nil,Cinder:nil,CephFS:nil,FC:nil,Flocker:nil,FlexVolume:nil,AzureFile:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Local:nil,StorageOS:nil,CSI:nil,},AccessModes:[ReadWriteOnce],ClaimRef:&ObjectReference{Kind:PersistentVolumeClaim,Namespace:default,Name:data-my-postgresql-0,UID:aa09cfb0-3684-4993-857b-009b28e89740,APIVersion:v1,ResourceVersion:7073,FieldPath:,},PersistentVolumeReclaimPolicy:Delete,StorageClassName:standard,MountOptions:[],VolumeMode:*Filesystem,NodeAffinity:nil,},Status:PersistentVolumeStatus{Phase:Released,Message:,Reason:,},}
I0624 10:34:37.971417       1 controller.go:1478] delete "pvc-aa09cfb0-3684-4993-857b-009b28e89740": volume deletion ignored: ignored because identity annotation on PV does not match ours
I0624 10:38:21.824671       1 controller.go:1472] delete "pvc-b2d45f1e-cb01-4f5e-a287-a75cc9cfeb90": started
I0624 10:38:21.825133       1 storage_provisioner.go:98] Deleting volume &PersistentVolume{ObjectMeta:{pvc-b2d45f1e-cb01-4f5e-a287-a75cc9cfeb90    a1bc9fbd-87ad-43c5-9f50-4df389ba69eb 41611 0 2024-06-21 14:34:34 +0000 UTC <nil> <nil> map[] map[hostPathProvisionerIdentity:7e2cccc6-578c-46bc-adec-448204cd4dee pv.kubernetes.io/provisioned-by:k8s.io/minikube-hostpath] [] [kubernetes.io/pv-protection]  [{storage-provisioner Update v1 2024-06-21 14:34:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:hostPathProvisionerIdentity":{},"f:pv.kubernetes.io/provisioned-by":{}}},"f:spec":{"f:accessModes":{},"f:capacity":{".":{},"f:storage":{}},"f:claimRef":{".":{},"f:apiVersion":{},"f:kind":{},"f:name":{},"f:namespace":{},"f:resourceVersion":{},"f:uid":{}},"f:hostPath":{".":{},"f:path":{},"f:type":{}},"f:persistentVolumeReclaimPolicy":{},"f:storageClassName":{},"f:volumeMode":{}}}} {kube-controller-manager Update v1 2024-06-24 10:38:21 +0000 UTC FieldsV1 {"f:status":{"f:phase":{}}}}]},Spec:PersistentVolumeSpec{Capacity:ResourceList{storage: {{8589934592 0} {<nil>}  BinarySI},},PersistentVolumeSource:PersistentVolumeSource{GCEPersistentDisk:nil,AWSElasticBlockStore:nil,HostPath:&HostPathVolumeSource{Path:/tmp/hostpath-provisioner/krakend-monitoring/prometheus-server,Type:*,},Glusterfs:nil,NFS:nil,RBD:nil,ISCSI:nil,Cinder:nil,CephFS:nil,FC:nil,Flocker:nil,FlexVolume:nil,AzureFile:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Local:nil,StorageOS:nil,CSI:nil,},AccessModes:[ReadWriteOnce],ClaimRef:&ObjectReference{Kind:PersistentVolumeClaim,Namespace:krakend-monitoring,Name:prometheus-server,UID:b2d45f1e-cb01-4f5e-a287-a75cc9cfeb90,APIVersion:v1,ResourceVersion:35385,FieldPath:,},PersistentVolumeReclaimPolicy:Delete,StorageClassName:standard,MountOptions:[],VolumeMode:*Filesystem,NodeAffinity:nil,},Status:PersistentVolumeStatus{Phase:Released,Message:,Reason:,},}
I0624 10:38:21.825766       1 controller.go:1478] delete "pvc-b2d45f1e-cb01-4f5e-a287-a75cc9cfeb90": volume deletion ignored: ignored because identity annotation on PV does not match ours
I0624 10:44:51.380454       1 controller.go:1332] provision "krakend-monitoring/prometheus-server" class "standard": started
I0624 10:44:51.469306       1 event.go:282] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"krakend-monitoring", Name:"prometheus-server", UID:"8dd61f54-33e2-415a-9463-db1443d955ae", APIVersion:"v1", ResourceVersion:"42155", FieldPath:""}): type: 'Normal' reason: 'Provisioning' External provisioner is provisioning volume for claim "krakend-monitoring/prometheus-server"
I0624 10:44:51.457450       1 storage_provisioner.go:61] Provisioning volume {&StorageClass{ObjectMeta:{standard    2e54b53f-4c2c-4f07-811b-23f4ae9344ff 1228 0 2024-06-19 11:01:02 +0000 UTC <nil> <nil> map[addonmanager.kubernetes.io/mode:EnsureExists] map[kubectl.kubernetes.io/last-applied-configuration:{"apiVersion":"storage.k8s.io/v1","kind":"StorageClass","metadata":{"annotations":{"storageclass.kubernetes.io/is-default-class":"true"},"labels":{"addonmanager.kubernetes.io/mode":"EnsureExists"},"name":"standard"},"provisioner":"k8s.io/minikube-hostpath"}
 storageclass.kubernetes.io/is-default-class:true] [] []  [{kubectl-client-side-apply Update storage.k8s.io/v1 2024-06-19 11:01:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:kubectl.kubernetes.io/last-applied-configuration":{},"f:storageclass.kubernetes.io/is-default-class":{}},"f:labels":{".":{},"f:addonmanager.kubernetes.io/mode":{}}},"f:provisioner":{},"f:reclaimPolicy":{},"f:volumeBindingMode":{}}}]},Provisioner:k8s.io/minikube-hostpath,Parameters:map[string]string{},ReclaimPolicy:*Delete,MountOptions:[],AllowVolumeExpansion:nil,VolumeBindingMode:*Immediate,AllowedTopologies:[]TopologySelectorTerm{},} pvc-8dd61f54-33e2-415a-9463-db1443d955ae &PersistentVolumeClaim{ObjectMeta:{prometheus-server  krakend-monitoring  8dd61f54-33e2-415a-9463-db1443d955ae 42155 0 2024-06-24 10:44:51 +0000 UTC <nil> <nil> map[app.kubernetes.io/component:server app.kubernetes.io/instance:prometheus app.kubernetes.io/managed-by:Helm app.kubernetes.io/name:prometheus app.kubernetes.io/part-of:prometheus app.kubernetes.io/version:v2.53.0 helm.sh/chart:prometheus-25.22.0] map[meta.helm.sh/release-name:prometheus meta.helm.sh/release-namespace:krakend-monitoring volume.beta.kubernetes.io/storage-provisioner:k8s.io/minikube-hostpath volume.kubernetes.io/storage-provisioner:k8s.io/minikube-hostpath] [] [kubernetes.io/pvc-protection]  [{helm Update v1 2024-06-24 10:44:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:meta.helm.sh/release-name":{},"f:meta.helm.sh/release-namespace":{}},"f:labels":{".":{},"f:app.kubernetes.io/component":{},"f:app.kubernetes.io/instance":{},"f:app.kubernetes.io/managed-by":{},"f:app.kubernetes.io/name":{},"f:app.kubernetes.io/part-of":{},"f:app.kubernetes.io/version":{},"f:helm.sh/chart":{}}},"f:spec":{"f:accessModes":{},"f:resources":{"f:requests":{".":{},"f:storage":{}}},"f:volumeMode":{}}}} {kube-controller-manager Update v1 2024-06-24 10:44:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:volume.beta.kubernetes.io/storage-provisioner":{},"f:volume.kubernetes.io/storage-provisioner":{}}}}}]},Spec:PersistentVolumeClaimSpec{AccessModes:[ReadWriteOnce],Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{storage: {{8589934592 0} {<nil>}  BinarySI},},},VolumeName:,Selector:nil,StorageClassName:*standard,VolumeMode:*Filesystem,DataSource:nil,},Status:PersistentVolumeClaimStatus{Phase:Pending,AccessModes:[],Capacity:ResourceList{},Conditions:[]PersistentVolumeClaimCondition{},},} nil} to /tmp/hostpath-provisioner/krakend-monitoring/prometheus-server
I0624 10:44:51.518052       1 controller.go:1439] provision "krakend-monitoring/prometheus-server" class "standard": volume "pvc-8dd61f54-33e2-415a-9463-db1443d955ae" provisioned
I0624 10:44:51.518245       1 controller.go:1456] provision "krakend-monitoring/prometheus-server" class "standard": succeeded
I0624 10:44:51.518298       1 volume_store.go:212] Trying to save persistentvolume "pvc-8dd61f54-33e2-415a-9463-db1443d955ae"
I0624 10:44:51.672053       1 volume_store.go:219] persistentvolume "pvc-8dd61f54-33e2-415a-9463-db1443d955ae" saved
I0624 10:44:51.674856       1 event.go:282] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"krakend-monitoring", Name:"prometheus-server", UID:"8dd61f54-33e2-415a-9463-db1443d955ae", APIVersion:"v1", ResourceVersion:"42155", FieldPath:""}): type: 'Normal' reason: 'ProvisioningSucceeded' Successfully provisioned volume pvc-8dd61f54-33e2-415a-9463-db1443d955ae
I0624 10:45:47.863348       1 controller.go:1472] delete "pvc-8dd61f54-33e2-415a-9463-db1443d955ae": started
I0624 10:45:47.863474       1 storage_provisioner.go:98] Deleting volume &PersistentVolume{ObjectMeta:{pvc-8dd61f54-33e2-415a-9463-db1443d955ae    f4efcaf7-b620-4448-a1b7-1ad6809bbf22 42395 0 2024-06-24 10:44:51 +0000 UTC <nil> <nil> map[] map[hostPathProvisionerIdentity:e012c0ff-5c6e-4bc2-bd2c-8ad57cd50986 pv.kubernetes.io/provisioned-by:k8s.io/minikube-hostpath] [] [kubernetes.io/pv-protection]  [{storage-provisioner Update v1 2024-06-24 10:44:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:hostPathProvisionerIdentity":{},"f:pv.kubernetes.io/provisioned-by":{}}},"f:spec":{"f:accessModes":{},"f:capacity":{".":{},"f:storage":{}},"f:claimRef":{".":{},"f:apiVersion":{},"f:kind":{},"f:name":{},"f:namespace":{},"f:resourceVersion":{},"f:uid":{}},"f:hostPath":{".":{},"f:path":{},"f:type":{}},"f:persistentVolumeReclaimPolicy":{},"f:storageClassName":{},"f:volumeMode":{}}}} {kube-controller-manager Update v1 2024-06-24 10:45:47 +0000 UTC FieldsV1 {"f:status":{"f:phase":{}}}}]},Spec:PersistentVolumeSpec{Capacity:ResourceList{storage: {{8589934592 0} {<nil>}  BinarySI},},PersistentVolumeSource:PersistentVolumeSource{GCEPersistentDisk:nil,AWSElasticBlockStore:nil,HostPath:&HostPathVolumeSource{Path:/tmp/hostpath-provisioner/krakend-monitoring/prometheus-server,Type:*,},Glusterfs:nil,NFS:nil,RBD:nil,ISCSI:nil,Cinder:nil,CephFS:nil,FC:nil,Flocker:nil,FlexVolume:nil,AzureFile:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Local:nil,StorageOS:nil,CSI:nil,},AccessModes:[ReadWriteOnce],ClaimRef:&ObjectReference{Kind:PersistentVolumeClaim,Namespace:krakend-monitoring,Name:prometheus-server,UID:8dd61f54-33e2-415a-9463-db1443d955ae,APIVersion:v1,ResourceVersion:42155,FieldPath:,},PersistentVolumeReclaimPolicy:Delete,StorageClassName:standard,MountOptions:[],VolumeMode:*Filesystem,NodeAffinity:nil,},Status:PersistentVolumeStatus{Phase:Released,Message:,Reason:,},}
I0624 10:45:47.988736       1 controller.go:1487] delete "pvc-8dd61f54-33e2-415a-9463-db1443d955ae": volume deleted
I0624 10:45:48.047949       1 controller.go:1537] delete "pvc-8dd61f54-33e2-415a-9463-db1443d955ae": persistentvolume deleted
I0624 10:45:48.048064       1 controller.go:1542] delete "pvc-8dd61f54-33e2-415a-9463-db1443d955ae": succeeded
I0624 10:49:37.970805       1 controller.go:1472] delete "pvc-aa09cfb0-3684-4993-857b-009b28e89740": started
I0624 10:49:37.970975       1 controller.go:1472] delete "pvc-b2d45f1e-cb01-4f5e-a287-a75cc9cfeb90": started
I0624 10:49:37.970984       1 storage_provisioner.go:98] Deleting volume &PersistentVolume{ObjectMeta:{pvc-b2d45f1e-cb01-4f5e-a287-a75cc9cfeb90    a1bc9fbd-87ad-43c5-9f50-4df389ba69eb 41611 0 2024-06-21 14:34:34 +0000 UTC <nil> <nil> map[] map[hostPathProvisionerIdentity:7e2cccc6-578c-46bc-adec-448204cd4dee pv.kubernetes.io/provisioned-by:k8s.io/minikube-hostpath] [] [kubernetes.io/pv-protection]  [{storage-provisioner Update v1 2024-06-21 14:34:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:hostPathProvisionerIdentity":{},"f:pv.kubernetes.io/provisioned-by":{}}},"f:spec":{"f:accessModes":{},"f:capacity":{".":{},"f:storage":{}},"f:claimRef":{".":{},"f:apiVersion":{},"f:kind":{},"f:name":{},"f:namespace":{},"f:resourceVersion":{},"f:uid":{}},"f:hostPath":{".":{},"f:path":{},"f:type":{}},"f:persistentVolumeReclaimPolicy":{},"f:storageClassName":{},"f:volumeMode":{}}}} {kube-controller-manager Update v1 2024-06-24 10:38:21 +0000 UTC FieldsV1 {"f:status":{"f:phase":{}}}}]},Spec:PersistentVolumeSpec{Capacity:ResourceList{storage: {{8589934592 0} {<nil>}  BinarySI},},PersistentVolumeSource:PersistentVolumeSource{GCEPersistentDisk:nil,AWSElasticBlockStore:nil,HostPath:&HostPathVolumeSource{Path:/tmp/hostpath-provisioner/krakend-monitoring/prometheus-server,Type:*,},Glusterfs:nil,NFS:nil,RBD:nil,ISCSI:nil,Cinder:nil,CephFS:nil,FC:nil,Flocker:nil,FlexVolume:nil,AzureFile:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Local:nil,StorageOS:nil,CSI:nil,},AccessModes:[ReadWriteOnce],ClaimRef:&ObjectReference{Kind:PersistentVolumeClaim,Namespace:krakend-monitoring,Name:prometheus-server,UID:b2d45f1e-cb01-4f5e-a287-a75cc9cfeb90,APIVersion:v1,ResourceVersion:35385,FieldPath:,},PersistentVolumeReclaimPolicy:Delete,StorageClassName:standard,MountOptions:[],VolumeMode:*Filesystem,NodeAffinity:nil,},Status:PersistentVolumeStatus{Phase:Released,Message:,Reason:,},}
I0624 10:49:37.971075       1 controller.go:1478] delete "pvc-b2d45f1e-cb01-4f5e-a287-a75cc9cfeb90": volume deletion ignored: ignored because identity annotation on PV does not match ours
I0624 10:49:37.970852       1 storage_provisioner.go:98] Deleting volume &PersistentVolume{ObjectMeta:{pvc-aa09cfb0-3684-4993-857b-009b28e89740    bfbd3236-def3-4a4c-8306-baf7242332fc 30187 0 2024-06-19 14:19:42 +0000 UTC <nil> <nil> map[] map[hostPathProvisionerIdentity:c540c508-262f-46d9-8ee0-a2360bf54623 pv.kubernetes.io/provisioned-by:k8s.io/minikube-hostpath] [] [kubernetes.io/pv-protection]  [{storage-provisioner Update v1 2024-06-19 14:19:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:hostPathProvisionerIdentity":{},"f:pv.kubernetes.io/provisioned-by":{}}},"f:spec":{"f:accessModes":{},"f:capacity":{".":{},"f:storage":{}},"f:claimRef":{".":{},"f:apiVersion":{},"f:kind":{},"f:name":{},"f:namespace":{},"f:resourceVersion":{},"f:uid":{}},"f:hostPath":{".":{},"f:path":{},"f:type":{}},"f:persistentVolumeReclaimPolicy":{},"f:storageClassName":{},"f:volumeMode":{}}}} {kube-controller-manager Update v1 2024-06-21 10:21:43 +0000 UTC FieldsV1 {"f:status":{"f:phase":{}}}}]},Spec:PersistentVolumeSpec{Capacity:ResourceList{storage: {{8589934592 0} {<nil>}  BinarySI},},PersistentVolumeSource:PersistentVolumeSource{GCEPersistentDisk:nil,AWSElasticBlockStore:nil,HostPath:&HostPathVolumeSource{Path:/tmp/hostpath-provisioner/default/data-my-postgresql-0,Type:*,},Glusterfs:nil,NFS:nil,RBD:nil,ISCSI:nil,Cinder:nil,CephFS:nil,FC:nil,Flocker:nil,FlexVolume:nil,AzureFile:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Local:nil,StorageOS:nil,CSI:nil,},AccessModes:[ReadWriteOnce],ClaimRef:&ObjectReference{Kind:PersistentVolumeClaim,Namespace:default,Name:data-my-postgresql-0,UID:aa09cfb0-3684-4993-857b-009b28e89740,APIVersion:v1,ResourceVersion:7073,FieldPath:,},PersistentVolumeReclaimPolicy:Delete,StorageClassName:standard,MountOptions:[],VolumeMode:*Filesystem,NodeAffinity:nil,},Status:PersistentVolumeStatus{Phase:Released,Message:,Reason:,},}
I0624 10:49:37.971147       1 controller.go:1478] delete "pvc-aa09cfb0-3684-4993-857b-009b28e89740": volume deletion ignored: ignored because identity annotation on PV does not match ours
I0624 11:04:37.970925       1 controller.go:1472] delete "pvc-aa09cfb0-3684-4993-857b-009b28e89740": started
I0624 11:04:37.971266       1 controller.go:1472] delete "pvc-b2d45f1e-cb01-4f5e-a287-a75cc9cfeb90": started
I0624 11:04:37.971291       1 storage_provisioner.go:98] Deleting volume &PersistentVolume{ObjectMeta:{pvc-aa09cfb0-3684-4993-857b-009b28e89740    bfbd3236-def3-4a4c-8306-baf7242332fc 30187 0 2024-06-19 14:19:42 +0000 UTC <nil> <nil> map[] map[hostPathProvisionerIdentity:c540c508-262f-46d9-8ee0-a2360bf54623 pv.kubernetes.io/provisioned-by:k8s.io/minikube-hostpath] [] [kubernetes.io/pv-protection]  [{storage-provisioner Update v1 2024-06-19 14:19:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:hostPathProvisionerIdentity":{},"f:pv.kubernetes.io/provisioned-by":{}}},"f:spec":{"f:accessModes":{},"f:capacity":{".":{},"f:storage":{}},"f:claimRef":{".":{},"f:apiVersion":{},"f:kind":{},"f:name":{},"f:namespace":{},"f:resourceVersion":{},"f:uid":{}},"f:hostPath":{".":{},"f:path":{},"f:type":{}},"f:persistentVolumeReclaimPolicy":{},"f:storageClassName":{},"f:volumeMode":{}}}} {kube-controller-manager Update v1 2024-06-21 10:21:43 +0000 UTC FieldsV1 {"f:status":{"f:phase":{}}}}]},Spec:PersistentVolumeSpec{Capacity:ResourceList{storage: {{8589934592 0} {<nil>}  BinarySI},},PersistentVolumeSource:PersistentVolumeSource{GCEPersistentDisk:nil,AWSElasticBlockStore:nil,HostPath:&HostPathVolumeSource{Path:/tmp/hostpath-provisioner/default/data-my-postgresql-0,Type:*,},Glusterfs:nil,NFS:nil,RBD:nil,ISCSI:nil,Cinder:nil,CephFS:nil,FC:nil,Flocker:nil,FlexVolume:nil,AzureFile:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Local:nil,StorageOS:nil,CSI:nil,},AccessModes:[ReadWriteOnce],ClaimRef:&ObjectReference{Kind:PersistentVolumeClaim,Namespace:default,Name:data-my-postgresql-0,UID:aa09cfb0-3684-4993-857b-009b28e89740,APIVersion:v1,ResourceVersion:7073,FieldPath:,},PersistentVolumeReclaimPolicy:Delete,StorageClassName:standard,MountOptions:[],VolumeMode:*Filesystem,NodeAffinity:nil,},Status:PersistentVolumeStatus{Phase:Released,Message:,Reason:,},}
I0624 11:04:37.971441       1 storage_provisioner.go:98] Deleting volume &PersistentVolume{ObjectMeta:{pvc-b2d45f1e-cb01-4f5e-a287-a75cc9cfeb90    a1bc9fbd-87ad-43c5-9f50-4df389ba69eb 41611 0 2024-06-21 14:34:34 +0000 UTC <nil> <nil> map[] map[hostPathProvisionerIdentity:7e2cccc6-578c-46bc-adec-448204cd4dee pv.kubernetes.io/provisioned-by:k8s.io/minikube-hostpath] [] [kubernetes.io/pv-protection]  [{storage-provisioner Update v1 2024-06-21 14:34:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:hostPathProvisionerIdentity":{},"f:pv.kubernetes.io/provisioned-by":{}}},"f:spec":{"f:accessModes":{},"f:capacity":{".":{},"f:storage":{}},"f:claimRef":{".":{},"f:apiVersion":{},"f:kind":{},"f:name":{},"f:namespace":{},"f:resourceVersion":{},"f:uid":{}},"f:hostPath":{".":{},"f:path":{},"f:type":{}},"f:persistentVolumeReclaimPolicy":{},"f:storageClassName":{},"f:volumeMode":{}}}} {kube-controller-manager Update v1 2024-06-24 10:38:21 +0000 UTC FieldsV1 {"f:status":{"f:phase":{}}}}]},Spec:PersistentVolumeSpec{Capacity:ResourceList{storage: {{8589934592 0} {<nil>}  BinarySI},},PersistentVolumeSource:PersistentVolumeSource{GCEPersistentDisk:nil,AWSElasticBlockStore:nil,HostPath:&HostPathVolumeSource{Path:/tmp/hostpath-provisioner/krakend-monitoring/prometheus-server,Type:*,},Glusterfs:nil,NFS:nil,RBD:nil,ISCSI:nil,Cinder:nil,CephFS:nil,FC:nil,Flocker:nil,FlexVolume:nil,AzureFile:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Local:nil,StorageOS:nil,CSI:nil,},AccessModes:[ReadWriteOnce],ClaimRef:&ObjectReference{Kind:PersistentVolumeClaim,Namespace:krakend-monitoring,Name:prometheus-server,UID:b2d45f1e-cb01-4f5e-a287-a75cc9cfeb90,APIVersion:v1,ResourceVersion:35385,FieldPath:,},PersistentVolumeReclaimPolicy:Delete,StorageClassName:standard,MountOptions:[],VolumeMode:*Filesystem,NodeAffinity:nil,},Status:PersistentVolumeStatus{Phase:Released,Message:,Reason:,},}
I0624 11:04:37.972267       1 controller.go:1478] delete "pvc-b2d45f1e-cb01-4f5e-a287-a75cc9cfeb90": volume deletion ignored: ignored because identity annotation on PV does not match ours
I0624 11:04:37.972169       1 controller.go:1478] delete "pvc-aa09cfb0-3684-4993-857b-009b28e89740": volume deletion ignored: ignored because identity annotation on PV does not match ours
I0624 11:19:37.972147       1 controller.go:1472] delete "pvc-b2d45f1e-cb01-4f5e-a287-a75cc9cfeb90": started
I0624 11:19:37.972342       1 controller.go:1472] delete "pvc-aa09cfb0-3684-4993-857b-009b28e89740": started
I0624 11:19:37.972366       1 storage_provisioner.go:98] Deleting volume &PersistentVolume{ObjectMeta:{pvc-aa09cfb0-3684-4993-857b-009b28e89740    bfbd3236-def3-4a4c-8306-baf7242332fc 30187 0 2024-06-19 14:19:42 +0000 UTC <nil> <nil> map[] map[hostPathProvisionerIdentity:c540c508-262f-46d9-8ee0-a2360bf54623 pv.kubernetes.io/provisioned-by:k8s.io/minikube-hostpath] [] [kubernetes.io/pv-protection]  [{storage-provisioner Update v1 2024-06-19 14:19:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:hostPathProvisionerIdentity":{},"f:pv.kubernetes.io/provisioned-by":{}}},"f:spec":{"f:accessModes":{},"f:capacity":{".":{},"f:storage":{}},"f:claimRef":{".":{},"f:apiVersion":{},"f:kind":{},"f:name":{},"f:namespace":{},"f:resourceVersion":{},"f:uid":{}},"f:hostPath":{".":{},"f:path":{},"f:type":{}},"f:persistentVolumeReclaimPolicy":{},"f:storageClassName":{},"f:volumeMode":{}}}} {kube-controller-manager Update v1 2024-06-21 10:21:43 +0000 UTC FieldsV1 {"f:status":{"f:phase":{}}}}]},Spec:PersistentVolumeSpec{Capacity:ResourceList{storage: {{8589934592 0} {<nil>}  BinarySI},},PersistentVolumeSource:PersistentVolumeSource{GCEPersistentDisk:nil,AWSElasticBlockStore:nil,HostPath:&HostPathVolumeSource{Path:/tmp/hostpath-provisioner/default/data-my-postgresql-0,Type:*,},Glusterfs:nil,NFS:nil,RBD:nil,ISCSI:nil,Cinder:nil,CephFS:nil,FC:nil,Flocker:nil,FlexVolume:nil,AzureFile:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Local:nil,StorageOS:nil,CSI:nil,},AccessModes:[ReadWriteOnce],ClaimRef:&ObjectReference{Kind:PersistentVolumeClaim,Namespace:default,Name:data-my-postgresql-0,UID:aa09cfb0-3684-4993-857b-009b28e89740,APIVersion:v1,ResourceVersion:7073,FieldPath:,},PersistentVolumeReclaimPolicy:Delete,StorageClassName:standard,MountOptions:[],VolumeMode:*Filesystem,NodeAffinity:nil,},Status:PersistentVolumeStatus{Phase:Released,Message:,Reason:,},}
I0624 11:19:37.972334       1 storage_provisioner.go:98] Deleting volume &PersistentVolume{ObjectMeta:{pvc-b2d45f1e-cb01-4f5e-a287-a75cc9cfeb90    a1bc9fbd-87ad-43c5-9f50-4df389ba69eb 41611 0 2024-06-21 14:34:34 +0000 UTC <nil> <nil> map[] map[hostPathProvisionerIdentity:7e2cccc6-578c-46bc-adec-448204cd4dee pv.kubernetes.io/provisioned-by:k8s.io/minikube-hostpath] [] [kubernetes.io/pv-protection]  [{storage-provisioner Update v1 2024-06-21 14:34:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:hostPathProvisionerIdentity":{},"f:pv.kubernetes.io/provisioned-by":{}}},"f:spec":{"f:accessModes":{},"f:capacity":{".":{},"f:storage":{}},"f:claimRef":{".":{},"f:apiVersion":{},"f:kind":{},"f:name":{},"f:namespace":{},"f:resourceVersion":{},"f:uid":{}},"f:hostPath":{".":{},"f:path":{},"f:type":{}},"f:persistentVolumeReclaimPolicy":{},"f:storageClassName":{},"f:volumeMode":{}}}} {kube-controller-manager Update v1 2024-06-24 10:38:21 +0000 UTC FieldsV1 {"f:status":{"f:phase":{}}}}]},Spec:PersistentVolumeSpec{Capacity:ResourceList{storage: {{8589934592 0} {<nil>}  BinarySI},},PersistentVolumeSource:PersistentVolumeSource{GCEPersistentDisk:nil,AWSElasticBlockStore:nil,HostPath:&HostPathVolumeSource{Path:/tmp/hostpath-provisioner/krakend-monitoring/prometheus-server,Type:*,},Glusterfs:nil,NFS:nil,RBD:nil,ISCSI:nil,Cinder:nil,CephFS:nil,FC:nil,Flocker:nil,FlexVolume:nil,AzureFile:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Local:nil,StorageOS:nil,CSI:nil,},AccessModes:[ReadWriteOnce],ClaimRef:&ObjectReference{Kind:PersistentVolumeClaim,Namespace:krakend-monitoring,Name:prometheus-server,UID:b2d45f1e-cb01-4f5e-a287-a75cc9cfeb90,APIVersion:v1,ResourceVersion:35385,FieldPath:,},PersistentVolumeReclaimPolicy:Delete,StorageClassName:standard,MountOptions:[],VolumeMode:*Filesystem,NodeAffinity:nil,},Status:PersistentVolumeStatus{Phase:Released,Message:,Reason:,},}
I0624 11:19:37.972652       1 controller.go:1478] delete "pvc-aa09cfb0-3684-4993-857b-009b28e89740": volume deletion ignored: ignored because identity annotation on PV does not match ours
I0624 11:19:37.972676       1 controller.go:1478] delete "pvc-b2d45f1e-cb01-4f5e-a287-a75cc9cfeb90": volume deletion ignored: ignored because identity annotation on PV does not match ours
I0624 11:34:37.972441       1 controller.go:1472] delete "pvc-aa09cfb0-3684-4993-857b-009b28e89740": started
I0624 11:34:37.972535       1 controller.go:1472] delete "pvc-b2d45f1e-cb01-4f5e-a287-a75cc9cfeb90": started
I0624 11:34:37.972514       1 storage_provisioner.go:98] Deleting volume &PersistentVolume{ObjectMeta:{pvc-aa09cfb0-3684-4993-857b-009b28e89740    bfbd3236-def3-4a4c-8306-baf7242332fc 30187 0 2024-06-19 14:19:42 +0000 UTC <nil> <nil> map[] map[hostPathProvisionerIdentity:c540c508-262f-46d9-8ee0-a2360bf54623 pv.kubernetes.io/provisioned-by:k8s.io/minikube-hostpath] [] [kubernetes.io/pv-protection]  [{storage-provisioner Update v1 2024-06-19 14:19:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:hostPathProvisionerIdentity":{},"f:pv.kubernetes.io/provisioned-by":{}}},"f:spec":{"f:accessModes":{},"f:capacity":{".":{},"f:storage":{}},"f:claimRef":{".":{},"f:apiVersion":{},"f:kind":{},"f:name":{},"f:namespace":{},"f:resourceVersion":{},"f:uid":{}},"f:hostPath":{".":{},"f:path":{},"f:type":{}},"f:persistentVolumeReclaimPolicy":{},"f:storageClassName":{},"f:volumeMode":{}}}} {kube-controller-manager Update v1 2024-06-21 10:21:43 +0000 UTC FieldsV1 {"f:status":{"f:phase":{}}}}]},Spec:PersistentVolumeSpec{Capacity:ResourceList{storage: {{8589934592 0} {<nil>}  BinarySI},},PersistentVolumeSource:PersistentVolumeSource{GCEPersistentDisk:nil,AWSElasticBlockStore:nil,HostPath:&HostPathVolumeSource{Path:/tmp/hostpath-provisioner/default/data-my-postgresql-0,Type:*,},Glusterfs:nil,NFS:nil,RBD:nil,ISCSI:nil,Cinder:nil,CephFS:nil,FC:nil,Flocker:nil,FlexVolume:nil,AzureFile:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Local:nil,StorageOS:nil,CSI:nil,},AccessModes:[ReadWriteOnce],ClaimRef:&ObjectReference{Kind:PersistentVolumeClaim,Namespace:default,Name:data-my-postgresql-0,UID:aa09cfb0-3684-4993-857b-009b28e89740,APIVersion:v1,ResourceVersion:7073,FieldPath:,},PersistentVolumeReclaimPolicy:Delete,StorageClassName:standard,MountOptions:[],VolumeMode:*Filesystem,NodeAffinity:nil,},Status:PersistentVolumeStatus{Phase:Released,Message:,Reason:,},}
I0624 11:34:37.973011       1 controller.go:1478] delete "pvc-aa09cfb0-3684-4993-857b-009b28e89740": volume deletion ignored: ignored because identity annotation on PV does not match ours
I0624 11:34:37.972551       1 storage_provisioner.go:98] Deleting volume &PersistentVolume{ObjectMeta:{pvc-b2d45f1e-cb01-4f5e-a287-a75cc9cfeb90    a1bc9fbd-87ad-43c5-9f50-4df389ba69eb 41611 0 2024-06-21 14:34:34 +0000 UTC <nil> <nil> map[] map[hostPathProvisionerIdentity:7e2cccc6-578c-46bc-adec-448204cd4dee pv.kubernetes.io/provisioned-by:k8s.io/minikube-hostpath] [] [kubernetes.io/pv-protection]  [{storage-provisioner Update v1 2024-06-21 14:34:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:hostPathProvisionerIdentity":{},"f:pv.kubernetes.io/provisioned-by":{}}},"f:spec":{"f:accessModes":{},"f:capacity":{".":{},"f:storage":{}},"f:claimRef":{".":{},"f:apiVersion":{},"f:kind":{},"f:name":{},"f:namespace":{},"f:resourceVersion":{},"f:uid":{}},"f:hostPath":{".":{},"f:path":{},"f:type":{}},"f:persistentVolumeReclaimPolicy":{},"f:storageClassName":{},"f:volumeMode":{}}}} {kube-controller-manager Update v1 2024-06-24 10:38:21 +0000 UTC FieldsV1 {"f:status":{"f:phase":{}}}}]},Spec:PersistentVolumeSpec{Capacity:ResourceList{storage: {{8589934592 0} {<nil>}  BinarySI},},PersistentVolumeSource:PersistentVolumeSource{GCEPersistentDisk:nil,AWSElasticBlockStore:nil,HostPath:&HostPathVolumeSource{Path:/tmp/hostpath-provisioner/krakend-monitoring/prometheus-server,Type:*,},Glusterfs:nil,NFS:nil,RBD:nil,ISCSI:nil,Cinder:nil,CephFS:nil,FC:nil,Flocker:nil,FlexVolume:nil,AzureFile:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Local:nil,StorageOS:nil,CSI:nil,},AccessModes:[ReadWriteOnce],ClaimRef:&ObjectReference{Kind:PersistentVolumeClaim,Namespace:krakend-monitoring,Name:prometheus-server,UID:b2d45f1e-cb01-4f5e-a287-a75cc9cfeb90,APIVersion:v1,ResourceVersion:35385,FieldPath:,},PersistentVolumeReclaimPolicy:Delete,StorageClassName:standard,MountOptions:[],VolumeMode:*Filesystem,NodeAffinity:nil,},Status:PersistentVolumeStatus{Phase:Released,Message:,Reason:,},}
I0624 11:34:37.973296       1 controller.go:1478] delete "pvc-b2d45f1e-cb01-4f5e-a287-a75cc9cfeb90": volume deletion ignored: ignored because identity annotation on PV does not match ours


==> storage-provisioner [c1963a358a7e] <==
I0624 09:33:44.919704       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0624 09:34:06.001799       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: connect: connection refused

